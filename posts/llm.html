<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.36" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://github.com/Genhiy/Genhiy.github.io/posts/llm.html"><meta property="og:site_name" content="Genhiy"><meta property="og:title" content="大模型常见问题及回答"><meta property="og:description" content="Transformer 基础篇 Transformer 的优势是什么 相比于 RNN： 并行化能力更强: RNN 每个步骤的输入依赖于前一个步骤的输出，只能顺序处理序列数据。相比之下，Transformer 可以同时处理序列中的所有元素，提高了训练过程的效率。 更好的处理长距离依赖:随着序列长度的增加，RNN 需要更多的步骤传递信息，可能导致信息丢失（..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Genhiy"><meta property="article:tag" content="无标签"><meta property="article:published_time" content="2024-06-28T00:00:00.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"大模型常见问题及回答","image":[""],"datePublished":"2024-06-28T00:00:00.000Z","dateModified":null,"author":[{"@type":"Person","name":"Genhiy"}]}</script><title>大模型常见问题及回答 | Genhiy</title><meta name="description" content="Transformer 基础篇 Transformer 的优势是什么 相比于 RNN： 并行化能力更强: RNN 每个步骤的输入依赖于前一个步骤的输出，只能顺序处理序列数据。相比之下，Transformer 可以同时处理序列中的所有元素，提高了训练过程的效率。 更好的处理长距离依赖:随着序列长度的增加，RNN 需要更多的步骤传递信息，可能导致信息丢失（...">
    <link rel="preload" href="/assets/style-k1BjDSx1.css" as="style"><link rel="stylesheet" href="/assets/style-k1BjDSx1.css">
    <link rel="modulepreload" href="/assets/app-SD3SAAIy.js"><link rel="modulepreload" href="/assets/llm.html-CIHIe611.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-tidiU0rX.js" as="script"><link rel="prefetch" href="/assets/intro.html-B_i218vL.js" as="script"><link rel="prefetch" href="/assets/index.html-DNShvliQ.js" as="script"><link rel="prefetch" href="/assets/index.html-D-A48mNS.js" as="script"><link rel="prefetch" href="/assets/link.html-1iy3tt92.js" as="script"><link rel="prefetch" href="/assets/index.html-DY1epRBw.js" as="script"><link rel="prefetch" href="/assets/bev.html-CeV7gtqB.js" as="script"><link rel="prefetch" href="/assets/cl.html-C96r9tEp.js" as="script"><link rel="prefetch" href="/assets/knn_ann.html-CuMxdsOS.js" as="script"><link rel="prefetch" href="/assets/lla.html-CISNwIsB.js" as="script"><link rel="prefetch" href="/assets/prompt.html-CdcQFNSo.js" as="script"><link rel="prefetch" href="/assets/reg.html-RRSn8o5e.js" as="script"><link rel="prefetch" href="/assets/240530.html-CAhTKGhA.js" as="script"><link rel="prefetch" href="/assets/240607.html-BUAhOsOh.js" as="script"><link rel="prefetch" href="/assets/240701.html-CKgOqmyR.js" as="script"><link rel="prefetch" href="/assets/240807.html-X3XM7WWC.js" as="script"><link rel="prefetch" href="/assets/index.html-CvmgSoQJ.js" as="script"><link rel="prefetch" href="/assets/article.html-B6KPC5ay.js" as="script"><link rel="prefetch" href="/assets/interview.html-D_XrWJoI.js" as="script"><link rel="prefetch" href="/assets/zym.html-osgdULY_.js" as="script"><link rel="prefetch" href="/assets/index.html-Dp0q4i0E.js" as="script"><link rel="prefetch" href="/assets/arch.html-DBOnodxl.js" as="script"><link rel="prefetch" href="/assets/meaning.html-D_qs1ONe.js" as="script"><link rel="prefetch" href="/assets/tech.html-a3dpab55.js" as="script"><link rel="prefetch" href="/assets/index.html-DQ7VdB93.js" as="script"><link rel="prefetch" href="/assets/retr.html-BZKiht6L.js" as="script"><link rel="prefetch" href="/assets/index.html-DfYwgbz2.js" as="script"><link rel="prefetch" href="/assets/mcjs.html-FW0UKQV8.js" as="script"><link rel="prefetch" href="/assets/yw.html-D7yga1VQ.js" as="script"><link rel="prefetch" href="/assets/index.html-BG0Z1yfI.js" as="script"><link rel="prefetch" href="/assets/ch3.1_gbdt_lr.html-DFnelnmu.js" as="script"><link rel="prefetch" href="/assets/index.html--rQxumjX.js" as="script"><link rel="prefetch" href="/assets/autodrive.html-BGBkIR6M.js" as="script"><link rel="prefetch" href="/assets/com.html-BsOnNj-D.js" as="script"><link rel="prefetch" href="/assets/extract.html-CRnCL8on.js" as="script"><link rel="prefetch" href="/assets/reco.html-DNHqwtzh.js" as="script"><link rel="prefetch" href="/assets/index.html-BFGyMqPq.js" as="script"><link rel="prefetch" href="/assets/index.html-eNFi2sJa.js" as="script"><link rel="prefetch" href="/assets/index.html-eKLagjxA.js" as="script"><link rel="prefetch" href="/assets/index.html-Eiun-OM-.js" as="script"><link rel="prefetch" href="/assets/a2u.html-BTvW8pvK.js" as="script"><link rel="prefetch" href="/assets/ad.html-Chw3GCi1.js" as="script"><link rel="prefetch" href="/assets/cold_start.html-Bcnh9k9I.js" as="script"><link rel="prefetch" href="/assets/zb.html-CwSArhSY.js" as="script"><link rel="prefetch" href="/assets/index.html-BQlLY_YS.js" as="script"><link rel="prefetch" href="/assets/sdm.html-DKwS78p9.js" as="script"><link rel="prefetch" href="/assets/index.html-BlDXS7oF.js" as="script"><link rel="prefetch" href="/assets/eges.html-9vaSgtt8.js" as="script"><link rel="prefetch" href="/assets/index.html-CAnvYZvm.js" as="script"><link rel="prefetch" href="/assets/ch2.5.1_tdm.html-D_4_qOey.js" as="script"><link rel="prefetch" href="/assets/ch2.5.2_dprt.html-C7H8fQVJ.js" as="script"><link rel="prefetch" href="/assets/index.html-IBTWsI4V.js" as="script"><link rel="prefetch" href="/assets/ch3.2.1_fm.html-BSN6Rs6K.js" as="script"><link rel="prefetch" href="/assets/ch3.2.3_dcn.html-Bxb5yKnn.js" as="script"><link rel="prefetch" href="/assets/index.html-y8YEByGf.js" as="script"><link rel="prefetch" href="/assets/ch2.2.1_fm.html-u2eiYnl-.js" as="script"><link rel="prefetch" href="/assets/ch2.2.2_w2v.html-CsOc6Swf.js" as="script"><link rel="prefetch" href="/assets/ch2.2.3_i2v.html-DG2hmeWc.js" as="script"><link rel="prefetch" href="/assets/ch2.2.4_dssm.html-DbwFgdVR.js" as="script"><link rel="prefetch" href="/assets/ch2.2.5_dssmaug.html-DYX1HYAX.js" as="script"><link rel="prefetch" href="/assets/index.html-CEZkio38.js" as="script"><link rel="prefetch" href="/assets/ch2.1.1_intro.html-BUY7pseO.js" as="script"><link rel="prefetch" href="/assets/ch2.1.2_usercf.html-C5Qwr-e3.js" as="script"><link rel="prefetch" href="/assets/ch2.1.3_itemcf.html-B6ekmApX.js" as="script"><link rel="prefetch" href="/assets/ch2.1.4_graph.html-tSjNbfPs.js" as="script"><link rel="prefetch" href="/assets/ch2.1.5_mf.html-Co18b8QF.js" as="script"><link rel="prefetch" href="/assets/AI_Baseline.html-Bet3imH0.js" as="script"><link rel="prefetch" href="/assets/Basic_AI.html-DVTqPX8s.js" as="script"><link rel="prefetch" href="/assets/index.html-U_tZNprg.js" as="script"><link rel="prefetch" href="/assets/activate_func.html-Dolnhh4Z.js" as="script"><link rel="prefetch" href="/assets/eval.html-C4w9XF1J.js" as="script"><link rel="prefetch" href="/assets/loss.html-CSH1NQoL.js" as="script"><link rel="prefetch" href="/assets/norm.html-C-bIkbdX.js" as="script"><link rel="prefetch" href="/assets/optimizer.html-DzqgdrAm.js" as="script"><link rel="prefetch" href="/assets/pe.html-BY-yp1w0.js" as="script"><link rel="prefetch" href="/assets/ALDVT.html-Bc1Tgf7b.js" as="script"><link rel="prefetch" href="/assets/KAN.html-BouzjBcn.js" as="script"><link rel="prefetch" href="/assets/Llama.html-CoCJnKay.js" as="script"><link rel="prefetch" href="/assets/index.html-PqNnoGzP.js" as="script"><link rel="prefetch" href="/assets/TransformerFAM.html-C8wJblms.js" as="script"><link rel="prefetch" href="/assets/VMamba.html-B5bVHJNz.js" as="script"><link rel="prefetch" href="/assets/keypoint.html-Chc_2PuP.js" as="script"><link rel="prefetch" href="/assets/pbv.html-CM6sUSNq.js" as="script"><link rel="prefetch" href="/assets/index.html-D1MLDWEW.js" as="script"><link rel="prefetch" href="/assets/ch3.3.1_wide_deep.html-Cp6U42Vj.js" as="script"><link rel="prefetch" href="/assets/index.html-fVo2QabC.js" as="script"><link rel="prefetch" href="/assets/ch3.4.1_din.html-D48h1H4q.js" as="script"><link rel="prefetch" href="/assets/index.html-DahPGpqP.js" as="script"><link rel="prefetch" href="/assets/ch4.1.1_new_reco.html-vE9bIf_s.js" as="script"><link rel="prefetch" href="/assets/ch4.1.2_da.html-C6AHNr7x.js" as="script"><link rel="prefetch" href="/assets/ch4.1.3_retr.html-BWUYBoiC.js" as="script"><link rel="prefetch" href="/assets/ch4.1.4_fet.html-GriQXKnu.js" as="script"><link rel="prefetch" href="/assets/ch4.1.5_rank.html-BnQIHfXy.js" as="script"><link rel="prefetch" href="/assets/index.html-K69DYeBv.js" as="script"><link rel="prefetch" href="/assets/hadoop.html-olvgU5oP.js" as="script"><link rel="prefetch" href="/assets/linux.html-CSZhJ0gz.js" as="script"><link rel="prefetch" href="/assets/lua.html-Bg6LISv4.js" as="script"><link rel="prefetch" href="/assets/redis.html-BaFYRpgZ.js" as="script"><link rel="prefetch" href="/assets/shell.html-7qaUMgM0.js" as="script"><link rel="prefetch" href="/assets/sql.html-ieNKSMAL.js" as="script"><link rel="prefetch" href="/assets/sql_p.html-vpplUAei.js" as="script"><link rel="prefetch" href="/assets/PythonLibs.html--z5uY_LO.js" as="script"><link rel="prefetch" href="/assets/index.html-B1pMXK6-.js" as="script"><link rel="prefetch" href="/assets/aicode.html-MZwYKA53.js" as="script"><link rel="prefetch" href="/assets/error.html-DrKgHr8q.js" as="script"><link rel="prefetch" href="/assets/partial.html-D2YuhDoq.js" as="script"><link rel="prefetch" href="/assets/py.html-D9l_ueJ_.js" as="script"><link rel="prefetch" href="/assets/torch.html-HO5hCIvI.js" as="script"><link rel="prefetch" href="/assets/index.html-D7zz3FC2.js" as="script"><link rel="prefetch" href="/assets/btq.html-B1HrSk-5.js" as="script"><link rel="prefetch" href="/assets/filter.html-Dw9MIH5w.js" as="script"><link rel="prefetch" href="/assets/hj.html-DMX4bADr.js" as="script"><link rel="prefetch" href="/assets/404.html-Bo3iH3zB.js" as="script"><link rel="prefetch" href="/assets/index.html-B5I0_eRY.js" as="script"><link rel="prefetch" href="/assets/index.html-ClJWy-CV.js" as="script"><link rel="prefetch" href="/assets/index.html-B3sfthn9.js" as="script"><link rel="prefetch" href="/assets/index.html-Bx1VnnG6.js" as="script"><link rel="prefetch" href="/assets/index.html-CW5OR89t.js" as="script"><link rel="prefetch" href="/assets/index.html-VjlupVOz.js" as="script"><link rel="prefetch" href="/assets/index.html-AguUwTBS.js" as="script"><link rel="prefetch" href="/assets/index.html-BsxImHrN.js" as="script"><link rel="prefetch" href="/assets/index.html-BnW7i74V.js" as="script"><link rel="prefetch" href="/assets/index.html-BeCOXmYv.js" as="script"><link rel="prefetch" href="/assets/index.html-C_f4UvuJ.js" as="script"><link rel="prefetch" href="/assets/index.html-CIXLK0mt.js" as="script"><link rel="prefetch" href="/assets/index.html-DZxNu885.js" as="script"><link rel="prefetch" href="/assets/index.html-0C-Akm2i.js" as="script"><link rel="prefetch" href="/assets/index.html-8Z--khoL.js" as="script"><link rel="prefetch" href="/assets/index.html-DiR7ahE2.js" as="script"><link rel="prefetch" href="/assets/index.html-CyOOn6lM.js" as="script"><link rel="prefetch" href="/assets/index.html-CFByz562.js" as="script"><link rel="prefetch" href="/assets/index.html-Cx-Rumv1.js" as="script"><link rel="prefetch" href="/assets/index.html-Mby2D2im.js" as="script"><link rel="prefetch" href="/assets/index.html-Bm01fnR_.js" as="script"><link rel="prefetch" href="/assets/index.html-DdkB5nDB.js" as="script"><link rel="prefetch" href="/assets/index.html-6s1ELHnU.js" as="script"><link rel="prefetch" href="/assets/index.html-_I-2v-6G.js" as="script"><link rel="prefetch" href="/assets/index.html-Q9Yiz4WS.js" as="script"><link rel="prefetch" href="/assets/index.html-TbqsRVEG.js" as="script"><link rel="prefetch" href="/assets/index.html-CI91DTOM.js" as="script"><link rel="prefetch" href="/assets/index.html-CNvbfS8Q.js" as="script"><link rel="prefetch" href="/assets/index.html-nf2VVhOY.js" as="script"><link rel="prefetch" href="/assets/index.html-DCBI8FCE.js" as="script"><link rel="prefetch" href="/assets/index.html-D8JNcOev.js" as="script"><link rel="prefetch" href="/assets/index.html-AVkByiCc.js" as="script"><link rel="prefetch" href="/assets/index.html-C2Tet-yA.js" as="script"><link rel="prefetch" href="/assets/index.html-CDXRWzYX.js" as="script"><link rel="prefetch" href="/assets/index.html-BxF5pz2C.js" as="script"><link rel="prefetch" href="/assets/index.html-CtIb7qcC.js" as="script"><link rel="prefetch" href="/assets/index.html-DQmTo3_8.js" as="script"><link rel="prefetch" href="/assets/index.html-EPFKxQP4.js" as="script"><link rel="prefetch" href="/assets/index.html-CkDfmAiz.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-SzV8tJDW.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="route-link vp-brand" href="/"><img class="vp-nav-logo" src="/light_removebg.svg" alt><!----><span class="vp-site-name hide-in-pad">Genhiy</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/" aria-label="首页"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>首页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/zsk/" aria-label="知识库"><span class="font-icon icon fa-fw fa-sm fas fa-laptop-code" style=""></span>知识库<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link active" href="/posts/" aria-label="博客"><span class="font-icon icon fa-fw fa-sm fas fa-newspaper" style=""></span>博客<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link nav-link" href="/essay/" aria-label="随笔"><span class="font-icon icon fa-fw fa-sm fas fa-wand-magic-sparkles" style=""></span>随笔<!----></a></div><div class="vp-nav-item hide-in-mobile"><a href="https://theme-hope.vuejs.press/zh/" rel="noopener noreferrer" target="_blank" aria-label="V2 文档" class="nav-link"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span>V2 文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/Genhiy/Genhiy.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/" aria-label="首页"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>首页<!----></a></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-laptop-code" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/zsk/" aria-label="知识库"><!---->知识库<!----></a><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">技术栈</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">人工智能</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">各领域思考</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/zsk/link.html" aria-label="常用知识链接"><!---->常用知识链接<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon fa-fw fa-sm fas fa-thumbs-up" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/rcmd/" aria-label="推荐系统"><!---->推荐系统<!----></a><!----></p><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">推荐系统概论</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">经典召回模型</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">经典排序模型</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">推荐系统实战</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">工业界应用梳理</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">推荐系统面经</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header active"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">博客</span><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/posts/bev.html" aria-label="BEV"><!---->BEV<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/posts/cl.html" aria-label="强化学习"><!---->强化学习<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/posts/knn_ann.html" aria-label="KNN和ANN"><!---->KNN和ANN<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/posts/lla.html" aria-label="抽象泄漏定律"><!---->抽象泄漏定律<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/posts/reg.html" aria-label="装饰器与注册器"><!---->装饰器与注册器<!----></a></li><li><a class="route-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/posts/llm.html" aria-label="大模型常见问题及回答"><!---->大模型常见问题及回答<!----></a></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/posts/prompt.html" aria-label="26条提示词书写策略"><!---->26条提示词书写策略<!----></a></li></ul></section></li><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/intro.html" aria-label="介绍页"><span class="font-icon icon fa-fw fa-sm fas fa-circle-info" style=""></span>介绍页<!----></a></li><li><a href="https://plugin-md-enhance.vuejs.press/zh/guide/content/revealjs/demo.html" rel="noopener noreferrer" target="_blank" aria-label="幻灯片" class="nav-link vp-sidebar-link vp-sidebar-page"><span class="font-icon icon fa-fw fa-sm fas fa-person-chalkboard" style=""></span>幻灯片<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->大模型常见问题及回答</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Genhiy</span></span><span property="author" content="Genhiy"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-06-28T00:00:00.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="vp-pageview waline-pageview-count" data-path="/posts/llm.html" data-page-key="/posts/llm.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 18 分钟</span><meta property="timeRequired" content="PT18M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category4 clickable" role="navigation">大模型</span><!--]--><meta property="articleSection" content="大模型"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag6 clickable" role="navigation">无标签</span><!--]--><meta property="keywords" content="无标签"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!--[--><!----><!--]--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#transformer-基础篇">Transformer 基础篇</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#大模型架构篇">大模型架构篇</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#模型训练与评估篇">模型训练与评估篇</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#模型微调篇">模型微调篇</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#模型加速篇">模型加速篇</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#模型优化篇">模型优化篇</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h3 id="transformer-基础篇" tabindex="-1"><a class="header-anchor" href="#transformer-基础篇"><span>Transformer 基础篇</span></a></h3><h4 id="transformer-的优势是什么" tabindex="-1"><a class="header-anchor" href="#transformer-的优势是什么"><span>Transformer 的优势是什么</span></a></h4><p>相比于 RNN：</p><ul><li>并行化能力更强: RNN 每个步骤的输入依赖于前一个步骤的输出，只能顺序处理序列数据。相比之下，Transformer 可以同时处理序列中的所有元素，提高了训练过程的效率。</li><li>更好的处理长距离依赖:随着序列长度的增加，RNN 需要更多的步骤传递信息，可能导致信息丢失（梯度消失/爆炸）。Transformer 通过自注意力机制直接对序列中任意两个位置的元素进行相关性建模，获取长程信息。</li><li>灵活的上下文捕获: RNN 只能以单向或双向（Bi-RNNs）的形式捕获上下文。相比之下,Transformer 的自注意力机制可以为每个元素提供全局上下文信息。</li></ul><p>相比于 CNN：</p><ul><li>全局注意力: CNN 通过固定大小的卷积核在局部邻域内提取特征，这限制了它捕获全局依赖的能力，需要多层堆叠。而Transformer 的自注意力机制关注输入序列中的任何部分，访问全局信息。</li><li>灵活性和可扩展性: 在处理长序列时，CNN 需要更多层的网络来捕捉长距离的依赖，这会导致参数数量增加，或者引入池化等下采样操作，损失了图像重要的位置信息。Transformer模型则具有处理长距离依赖的能力，而且不需要随着输入长度的增加而显著增加模型大小。</li></ul><h4 id="为什么-q-和-k-要使用不同的权重矩阵进行线性变换" tabindex="-1"><a class="header-anchor" href="#为什么-q-和-k-要使用不同的权重矩阵进行线性变换"><span>为什么 Q 和 K 要使用不同的权重矩阵进行线性变换?</span></a></h4><ul><li>如果 Q 和 K 一样，矩阵乘积的结果是一个对称矩阵，表达能力不强</li><li>对角线上的值比较大，导致每个位置过分关注自己，忽略了上下文</li><li>增加参数量，增强模型表达能力</li></ul><h4 id="为什么要多头" tabindex="-1"><a class="header-anchor" href="#为什么要多头"><span>为什么要多头?</span></a></h4><ul><li>多头注意力使用多个维度较低的子空间分别进行学习，多个头可以分别关注到不同的特征，增强了表达能力，例如有的头关注语法信息，有的头关注知识内容，有的关注近距离，这样会减少信息的损失，提升模型容量</li><li>减少了注意力权重对角线值（局部信息）过大的情况，分散了注意力，防止只关注局部而忽略了上下文</li></ul><h4 id="为什么要进行-scale" tabindex="-1"><a class="header-anchor" href="#为什么要进行-scale"><span>为什么要进行 scale</span></a></h4><p>处理长序列时，输入信息维度 d 比较大，点积有比较大的方差，导致 softmax 函数的梯度 比较小；通过切分之后，缩小方差，防止梯度消失。</p><h3 id="大模型架构篇" tabindex="-1"><a class="header-anchor" href="#大模型架构篇"><span>大模型架构篇</span></a></h3><h4 id="bert-和-gpt-的区别" tabindex="-1"><a class="header-anchor" href="#bert-和-gpt-的区别"><span>Bert 和 GPT 的区别：</span></a></h4><ul><li>训练方法：BERT 的训练目标是遮蔽语言模型（MLM: Masked Language Model）和下一句预测（NSP:Next Sentence Prediction）。在 MLM 中，随机抽取输入句子中 15%文本，选中的文本中，80%被替换为 mask，10%的文本被替换为其它 token，10%不变。得到最后一层每个 token 的隐状态，在[mask]头部添加 mlp 映射词表上的概率分布。在 NSP 中，输入为两个句子，取 [cls]的隐状态，通过 mlp 做 3 分类（蕴含、矛盾、中性关系）。GPT 的训练目标是基于之前的单词来预测下一个单词。</li><li>方向性：BERT 使用双向上下文来进行训练。GPT 使用单向（从左到右）的上下文来进行训练。</li><li>架构设计：BERT 主要是一个编码器架构，它只使用了 Transformer 的编码器部分。GPT 是一个解码器架构，使用了 Transformer 的解码器部分，但不使用编码器-解码器注意力机制。</li><li>应用场景：BERT 更擅长理解上下文，因此在需要文本理解的下游任务，如情感分析、命名实体识别等任务上表现出色。GPT 更擅长生成文本，因此在需要生成连贯文本的应用上更为擅长，如文本生成、对话系统、甚至某些创造性写作的任务上。Encoder-Decoder 模型包括 Seq2Seq 模型，它在机器翻译、文本摘要等领域被广泛应用。</li></ul><h4 id="gpt1-gpt4-的主要技术要点" tabindex="-1"><a class="header-anchor" href="#gpt1-gpt4-的主要技术要点"><span>GPT1-GPT4 的主要技术要点</span></a></h4><ul><li>GPT1 <ul><li>基于 Transformer 架构，可以捕捉长距离依赖性，并且具有高效的并行性。</li><li>训练 GPT1 分为两个阶段：预训练和微调（fine-tuning）。在预训练阶段，GPT-1 使用了大量的无标注文本数据集，例如维基百科和网页文本等。通过最大化预训练数据集上的 log likelihood 来训练模型参数。在微调阶段，GPT-1 将预训练模型的参数用于特定的自然语言处理任务，如文本分类和问答系统等。</li></ul></li><li>GPT2 <ul><li>输入：Task conditioning</li><li>数据：只爬取人工筛选过的 web 数据，且数据量更大</li><li>输入表征：GPT-2 综合考虑了 OOV 问题和词表过大的问题，使用了 BPE 算法</li><li>模型结构：调整 Transformer 的 decoder（将 LN 层移动到 block 的输入位置并且在最后一个 self-attention 之后加了一层 LN）、layer 增加。</li><li>训练：去掉了 Fine-tune 部分，使用了完全的无监督训练。GPT-2 具备了零样本学习的能力</li></ul></li><li>GPT3 <ul><li>更多的训练数据量和模型参数量</li><li>引入了稀疏注意力机制和自适应注意力跨度（根据不同的任务和场景灵活地调整自己的注意力机制）。稀疏注意力机制的基本思想是，不需要计算每个位置与其他所有位置的相关性，而是只选择一部分重要或者相关的位置进行计算。稀疏模式包括滑动窗口、空洞卷积、局部区域等，来确定每个位置需要关注的其他位置。自适应注意力跨度的主要思想是，对于较短或较简单的文本，模型可以使用较大的注意力跨度来捕捉全局的语义信息；而对于较长或较复杂的文本，模型可以使用较小的注意力跨度来关注局部的语法结构和细节</li></ul></li></ul><p>InstructGPT：RLHF：（1）SFT （2）RM （3）PPO</p><h4 id="llama-的技术改进" tabindex="-1"><a class="header-anchor" href="#llama-的技术改进"><span>LLaMA 的技术改进</span></a></h4><ul><li>使用 SwiGLU 激活函数替换 ReLU 以提高性能：它由 GLU 和 Swish 函数两个部分组成，GLU 的输入向量 x 分别经过两个 linear 层，其中一个需要经过非线性激活函数，然后将两者对应元素相乘。SwiGLU 是采用 Swish 作为激活函数的 GLU 变体。</li><li>使用了旋转位置编码（RoPE）替换正余弦位置编码：RoPE 是一种相对位置编码方法，核心思想是利用旋转矩阵乘法来模拟序列中元素的位置偏移。对于每个位置 pos，构造一个旋转矩阵，该矩阵在多维空间中对应于维度成对的旋转。相比于绝对位置编码，RoPE 的优势在于：(a) 保持线性关系：RoPE 保留了位置偏移的线性关系，这有助于模型更有效地捕捉序列中元素的相对位置关系。(b)泛化能力：RoPE 依赖于相对位置而非绝对位置，它能够更好地处理训练时未见过的序列长度，提供更好的泛化能力。</li><li>前置层归一化（Pre-normalization）并使用 RMSNorm 归一化函数：LN 计算特征的均值和标准差，而均方根(RMS)归一化直接使用均方根（RMS）进行归一化，去掉了均值计算，优势在于提高了计算效率、某些模型中训练更稳定</li><li>AdamW 优化器：AdamW 引入了额外的权重衰减项，明确地在梯度更新步骤中加入了权重衰减，从而避免了 Adam 优化器对权重衰减的不准确估计问题。</li><li>更长的上下文长度 4k</li><li>Grouped-Query Attention</li></ul><h3 id="模型训练与评估篇" tabindex="-1"><a class="header-anchor" href="#模型训练与评估篇"><span>模型训练与评估篇</span></a></h3><h4 id="rlhf-的几个阶段的-loss" tabindex="-1"><a class="header-anchor" href="#rlhf-的几个阶段的-loss"><span>RLHF 的几个阶段的 Loss</span></a></h4><p>RM:</p><p>PPO:一般 PPO 阶段需要 4 个模型：两个训练两个推理</p><ul><li>Actor 模型：由 SFT 初始化，参数训练，进行强化学习的主模型，也是我们想要最终 获得的模型，它不断产生 action 并被 Critic 模型所评价，计算 loss 并训练。</li><li>Reference 模型，来自于 SFT 模型，不更新参数，约束 KL 项，防止 Actor 过度偏离</li><li>Reward 模型，来自于 RM 模型，不更新参数，用于打分</li><li>Critic 模型，由 RM 模型初始化，参数训练，用于预测 Actor 模型的收益</li></ul><p>DPO:</p><p>上面的 loss 可以简化为：</p><p>优化目标是让本简化公式最大，即我们希望左半部分和右半部分的 margin 越大越好，左半部分的含义是 good response 相较于没训练之前的累积概率差值，右半部分代表 bad response 相较于没训练之前的累计概率差值，如果这个差值，即 margin 变大了，就意味着：</p><ul><li>左边变大，右边变小，理想情况，good response 概率提升，bad response 概率下降</li><li>左边变小，右边更小，good response 概率下降，但是 bad response 概率下降的更多，生成 的时候还是倾向于 good response</li><li>左边变的更大，右边只大了一点点，</li></ul><h4 id="rlhf-过程中-rm-随着训练过程得分越来越高-效果就一定好吗-有没有极端情况" tabindex="-1"><a class="header-anchor" href="#rlhf-过程中-rm-随着训练过程得分越来越高-效果就一定好吗-有没有极端情况"><span>RLHF 过程中 RM 随着训练过程得分越来越高，效果就一定好吗？有没有极端情况？</span></a></h4><p>这里有几种可能的极端情况：</p><ul><li>过拟合：如果 RM 过度拟合于训练数据，它可能在训练集上得到很高的得分，但在未见过的数据上表现不佳。这意味着虽然 RM 得分很高，模型的泛化能力却可能很差。</li><li>奖励函数的误导：如果 RM 的设计或训练过程中引入了偏差，它可能会引导策略模型（Policy Model）学习错误的行为。例如，如果人类反馈中系统性地偏好某种行为，即使这种行为并不理想，RM 也可能学会高估这些行为的价值。</li><li>探索不足：在 RL 中，充分的探索对于发现最优策略至关重要。如果 RL 过程过于依赖 RM得分而减少了探索，可能导致策略收敛于次优解。</li></ul><h4 id="在训练语言大模型时-除了-loss-之外-如何在训练过程中监控模型能力" tabindex="-1"><a class="header-anchor" href="#在训练语言大模型时-除了-loss-之外-如何在训练过程中监控模型能力"><span>在训练语言大模型时，除了 loss 之外，如何在训练过程中监控模型能力？</span></a></h4><p>在训练语言模型时，监控模型能力的方式不仅限于观察 loss 下降。以下是一些其他可以监控以了解模型性能的关键指标和方法：</p><ul><li>验证集上的性能：除了训练集的 loss，还应该定期在验证集上计算模型的 loss 和其他性能指标（如准确率、BLEU 得分等，具体取决于任务）。这有助于监控模型是否出现过拟合。</li><li>样本推理：定期从验证集或测试集中选取样本进行推理，并检查模型的输出。这可以帮助直观地了解模型在实际任务上的表现。</li><li>设计对抗样本并测试：在了解模型的一般输出后，针对模型的已知弱点或典型错误，手动设计对抗样本，例如在文本中引入歧义、干扰信息或语法错误等，然后将对抗样本输入到语言模型中，查看模型在哪些类型的对抗攻击下更容易失败，针对缺陷改进模型。</li><li>A/B 测试：如果条件允许，可以在线上环境中对比新旧模型的表现，看看实际应用中哪个模型更胜一筹。</li><li>困惑度(Perplexity)：困惑度衡量了语言模型预测下一个词的不确定性，困惑度越低，表明模型能够正确预测出下一个词概率越高，表明模型越好。</li><li>自定义评估指标：根据模型的应用场景，可能需要定义特定的评估指标来更好地衡量模型的性能。</li></ul><h3 id="模型微调篇" tabindex="-1"><a class="header-anchor" href="#模型微调篇"><span>模型微调篇</span></a></h3><h4 id="lora" tabindex="-1"><a class="header-anchor" href="#lora"><span>LoRA</span></a></h4><ul><li>背景：全量参数 Fine-tune 需要调整模型全部参数，随着预训练模型规模的不断扩大，全量Fine-tune 的资源压力也倍增。</li><li>原理：预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和全量微调能起到相同的效果。</li><li>做法：冻结原始模型的参数，插入两个低秩矩阵，把大矩阵拆成两个小矩阵的乘法，仅训练新插入的参数。将可微调参数分配到多种类型权重矩阵中，而不应该用更大的秩单独微调某种类型的权重矩阵。</li><li>优缺点：优势：减少了需要更新的参数量、保留预训练知识避免灾难性遗忘；缺点：可能在需要大幅度模型调整的任务上表现不佳、依赖于预训练模型的质量</li></ul><h4 id="lora-相比于全参数训练-为什么可以提升训练效率" tabindex="-1"><a class="header-anchor" href="#lora-相比于全参数训练-为什么可以提升训练效率"><span>LoRA 相比于全参数训练，为什么可以提升训练效率？</span></a></h4><ul><li>计算量上，LoRA 在主干部分全连接层增加了 LoRA 旁路，参数量略有增加</li><li>显存上，参数、梯度、激活值也略有增加，但是优化器不需要存储原模型的参数，优化器状态节省显存较多。因此，LoRA 提升训练效率是因为：（1）优化器部分需要显存减少。（2）主干网络不用更 新，可以量化到 int8 等</li></ul><h4 id="dora" tabindex="-1"><a class="header-anchor" href="#dora"><span>DoRA</span></a></h4><ul><li>DoRA 的主要思想是将预训练权重分解为幅度（magnitude）和方向（direction），并利用 LoRA 来微调方向矩阵。</li><li>DoRA 的优势：LoRA 通常会等比例增减幅度和方向，DoRA 通过将预训练权重矩阵分解为 幅度和方向，能够更接近全量微调的效果。</li></ul><h3 id="模型加速篇" tabindex="-1"><a class="header-anchor" href="#模型加速篇"><span>模型加速篇</span></a></h3><h4 id="估计全量微调一个-7b-参数量的大模型-需要多少显存" tabindex="-1"><a class="header-anchor" href="#估计全量微调一个-7b-参数量的大模型-需要多少显存"><span>估计全量微调一个 7B 参数量的大模型，需要多少显存</span></a></h4><ul><li>模型的参数通常需要存储为 32 位浮点数（float32），每个参数占用 4 字节。参数: 28 GB</li><li>激活值: 取决于具体模型和批处理大小，假设为参数的 2 倍，则为 56 GB。梯度: 28 GB</li><li>优化器状态: 56 GB (如果使用 Adam)</li><li>总计 168G</li></ul><h4 id="解决显存不够的方法有哪些" tabindex="-1"><a class="header-anchor" href="#解决显存不够的方法有哪些"><span>解决显存不够的方法有哪些？</span></a></h4><ul><li>减小 batch：降低 batch 可以直接减少每次迭代所需的显存量。虽然这可能会影响训练的收敛速度和稳定性，但它是最直接的减少显存使用的方法。</li><li>梯度累积：梯度累积允许你使用小批次执行多个前向和反向传播，累积梯度，然后一次性更新权重。</li><li>混合精度训练：同时使用 FP16 和 FP32 进行计算，可以减少显存使用。</li><li>模型并行：将模型的不同部分放在不同的 GPU 上。</li><li>数据并行：数据并行保持模型在每个设备上完整复制，但将不同的数据批次发送到不同的设备。每个设备独立计算前向和后向传递，然后跨设备聚合梯度。这不仅可以扩展训练过程，还可以通过多个 GPU 分摊显存负担。</li><li>优化器状态分片：对于使用像 Adam 这样的优化器，优化器状态（如一阶和二阶矩估计）可以占用大量显存。通过分片优化器状态，每个 GPU 只存储并更新模型参数的一个子集的状态，可以减少每个 GPU 的显存使用</li></ul><h4 id="flash-attention-机制" tabindex="-1"><a class="header-anchor" href="#flash-attention-机制"><span>flash attention 机制</span></a></h4><p>Motivation: 大模型不能处理长 token，计算复杂度和空间复杂度随 token 长度 N 呈二次方增长。</p><p>模拟 attention 的时间复杂度：</p><p>(1)计算 QKV 矩阵：O(n<em>d</em>d) (2)计算 QKT：O(n<em>d</em>n) (3)计算 att×V：O(n<em>n</em>d)，因此注意力机制的总时间复杂度为 O(n<em>d2+n2</em>d)，随 n 二次增长</p><p>Flash Attention：旨在提高注意力的计算效率，Flash Attention 的核心思想是避免显式地计算和存储整个注意力矩阵。具体操作包括：</p><ul><li>分块处理：Flash Attention 将输入序列分成更小的块，然后在这些块上并行计算自注意力。减少了同时需要保持在内存中的数据量，允许更高效的硬件加速。</li><li>逐元素计算：采用逐元素的计算方法。在传统的自注意力机制中，我们通常会计算完整的注意力矩阵，然后再与 Value 矩阵相乘。而 Flash Attention 在计算过程中逐步累积最终的加权 Value 结果。对于每一个输出位置，Flash Attention 立即使用其对应的注意力得分来更新加权 Value，而不是先计算整个得分矩阵。</li></ul><h4 id="fp32-和-fp16-的区别-混合精度的原理" tabindex="-1"><a class="header-anchor" href="#fp32-和-fp16-的区别-混合精度的原理"><span>fp32 和 fp16 的区别，混合精度的原理</span></a></h4><p>FP32 是一种使用 32 位来存储一个浮点数的格式，提供了广泛的动态范围和较高的精度，FP16 是一种使用 16 位来存储一个浮点数的格式，相较于 FP32，FP16 有更小的动态范围和精度，但占用的存储和计算资源更少。</p><p>混合精度训练结合了 FP32 和 FP16，旨在利用 FP16 计算的高效性和 FP32 计算的高精度性。在混合精度训练中，大部分前向和后向传播的计算使用 FP16 进行，这样可以减少计算所需的时间和内存。但是，为了维持训练的稳定性和模型的最终精度，关键的部分，比如权重更新，仍然使用 FP32 执行。</p><p>具体的说：</p><ul><li>fp16: 激活值、前向计算中的参数、反向计算中的梯度</li><li>fp32: Adam 状态、参数更新时的参数、梯度</li><li>参数（权重）：参数通常在 FP32 精度下维护和更新。但是在计算前向激活值和反向梯度计算时，使用权重的 FP16 副本</li><li>梯度：梯度在反向传播中首先以 FP16 计算，因为激活值和权重在前向传播中使用 FP16。为了避免梯度过小而在 FP16 表示中下溢到 0，通常会应用梯度缩放（loss scaling）。之后，在更新权重之前，这些梯度会被转换回 FP32，确保更新的精度。</li><li>激活值：激活值在前向传播时以 FP16 计算以减少内存使用和加速计算。</li><li>Adam 状态（即优化器状态）：Adam 优化器维护每个参数的一阶和二阶矩估计，这些状态对于稳定和准确的权重更新非常关键。因此，这些状态通常以 FP32 来存储的，以避免数值不稳定性。</li></ul><h3 id="模型优化篇" tabindex="-1"><a class="header-anchor" href="#模型优化篇"><span>模型优化篇</span></a></h3><h4 id="什么是-llms-复读机问题" tabindex="-1"><a class="header-anchor" href="#什么是-llms-复读机问题"><span>什么是 LLMs 复读机问题？</span></a></h4><p>LLMs 复读机问题指的是大型语言模型（LLMs）在生成文本时倾向于以过度频繁的方式重复相同的句子或短语，输出缺乏多样性和创造性。 复读机问题可能出现的原因包括：</p><ol><li>数据偏差：大型语言模型通常是通过预训练阶段使用大规模无标签数据进行训练的。如果训练数据中存在大量的重复文本或者某些特定的句子或短语出现频率较高，模型在生成文本时可能会倾向于复制这些常见的模式。</li><li>训练目标的限制：大型语言模型的训练通常是基于自监督学习的方法，通过预测下一个词或掩盖词来学习语言模型。这样的训练目标可能使得模型更倾向于生成与输入相似的文本，导致复读机问题的出现。</li><li>训练数据缺乏多样性：虽然大型语言模型可以处理大规模的数据，但如果训练数据中缺乏多样性的语言表达和语境，模型可能无法学习到足够的多样性和创造性，导致复读机问题的出现。</li></ol><p>为了解决复读机问题，可以采取以下策略：</p><ol><li>增强训练数据的多样性：尽量使用多样性的语料库来训练模型、注意数据来源配比</li><li>引入噪声：在生成文本时，可以引入一些随机性或噪声，例如通过采样不同的词或短语，或者引入随机的变换操作，以增加生成文本的多样性。</li><li>增大温度参数：温度参数控制生成文本的多样性。当 T &gt; 1 时，logits 被除以一个大于 1的数，减少了不同概率之间的差异，概率分布变得更“平坦”,使得低概率词被选中的机会增加，生成的文本多样性更高，但可能牺牲一些连贯性或准确性。当 T&lt;1 时，概率分布变得更“尖锐”，即模型更倾向于选择概率较高的词，从而生成更确定性和一致性的文本。当 T=1 时，不改变原始概率分布，模型按照训练时学到的分布进行预测。</li><li>后处理和过滤：对生成的文本进行后处理和过滤，去除重复的句子或短语。</li></ol></div><!--[--><!----><!--]--><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a href="https://github.com/Genhiy/Genhiy.github.io/edit/main/src/posts/llm.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link vp-meta-label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="vp-meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a class="route-link nav-link prev" href="/posts/reg.html" aria-label="装饰器与注册器"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->装饰器与注册器</div></a><a class="route-link nav-link next" href="/posts/prompt.html" aria-label="26条提示词书写策略"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">26条提示词书写策略<!----></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">努力把生活过成诗</div><div class="vp-copyright">Copyright © 2024 Genhiy </div></footer></div><!--]--><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-SD3SAAIy.js" defer></script>
  </body>
</html>
