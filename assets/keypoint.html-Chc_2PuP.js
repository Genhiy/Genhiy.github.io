import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o as t,c as n,e as a}from"./app-SD3SAAIy.js";const r={},o=a('<h2 id="记忆模块" tabindex="-1"><a class="header-anchor" href="#记忆模块"><span>记忆模块</span></a></h2><h3 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h3><p>TransformerFAM: Feedback attention is working memory​：</p><p>核心思想是利用反馈循环（feedback loop）来使网络能够关注其自身的潜在表示（latent representations）。</p><p>简单来说就是添加了两个记忆token用于存储之前的信息。所用的是attention和concat，也就是先将之前的记忆和当前的输入进行concat和layer norm再计算attention，最后加和。</p><h3 id="mamba" tabindex="-1"><a class="header-anchor" href="#mamba"><span>Mamba</span></a></h3><p>HiPPO: Recurrent Memory with Optimal Polynomial Projections</p><p>逐渐维护序列的记忆表示。用HiPPO矩阵去做信息压缩，赋予远端记忆和近端记忆不同的权重。</p><h2 id="模型轻量化" tabindex="-1"><a class="header-anchor" href="#模型轻量化"><span>模型轻量化</span></a></h2><h3 id="快速训练" tabindex="-1"><a class="header-anchor" href="#快速训练"><span>快速训练</span></a></h3><p>TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding：</p><p>核心思想是用一种更轻量化的方法去表示attention过程中的KV以降低缓存。类似于知识蒸馏，用一个更简单的表示去拟合一个复杂模型，以降低模型中存在的冗余。</p><p>论文提出注意力稀疏性（Attention Sparsity）：KV缓存中存在大量冗余，发现使用部分KV缓存作为草图缓存进行自我推测，足以实现高接受率。</p>',13),i=[o];function p(s,c){return t(),n("div",null,i)}const h=e(r,[["render",p],["__file","keypoint.html.vue"]]),d=JSON.parse('{"path":"/zsk/ai/paper/keypoint.html","title":"keypoint","lang":"zh-CN","frontmatter":{"date":"2024-04-24T00:00:00.000Z","title":"keypoint","author":"Genhiy","index":false,"article":false,"order":-1,"category":["论文阅读"],"tag":["无标签"],"description":"记忆模块 Transformer TransformerFAM: Feedback attention is working memory​： 核心思想是利用反馈循环（feedback loop）来使网络能够关注其自身的潜在表示（latent representations）。 简单来说就是添加了两个记忆token用于存储之前的信息。所用的是atten...","head":[["meta",{"property":"og:url","content":"https://github.com/Genhiy/Genhiy.github.io/zsk/ai/paper/keypoint.html"}],["meta",{"property":"og:site_name","content":"Genhiy"}],["meta",{"property":"og:title","content":"keypoint"}],["meta",{"property":"og:description","content":"记忆模块 Transformer TransformerFAM: Feedback attention is working memory​： 核心思想是利用反馈循环（feedback loop）来使网络能够关注其自身的潜在表示（latent representations）。 简单来说就是添加了两个记忆token用于存储之前的信息。所用的是atten..."}],["meta",{"property":"og:type","content":"website"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Genhiy"}],["meta",{"property":"article:tag","content":"无标签"}],["meta",{"property":"article:published_time","content":"2024-04-24T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"WebPage\\",\\"name\\":\\"keypoint\\",\\"description\\":\\"记忆模块 Transformer TransformerFAM: Feedback attention is working memory​： 核心思想是利用反馈循环（feedback loop）来使网络能够关注其自身的潜在表示（latent representations）。 简单来说就是添加了两个记忆token用于存储之前的信息。所用的是atten...\\"}"]]},"headers":[{"level":2,"title":"记忆模块","slug":"记忆模块","link":"#记忆模块","children":[{"level":3,"title":"Transformer","slug":"transformer","link":"#transformer","children":[]},{"level":3,"title":"Mamba","slug":"mamba","link":"#mamba","children":[]}]},{"level":2,"title":"模型轻量化","slug":"模型轻量化","link":"#模型轻量化","children":[{"level":3,"title":"快速训练","slug":"快速训练","link":"#快速训练","children":[]}]}],"git":{},"readingTime":{"minutes":1.02,"words":306},"filePathRelative":"zsk/ai/paper/keypoint.md","localizedDate":"2024年4月24日","excerpt":"<h2>记忆模块</h2>\\n<h3>Transformer</h3>\\n<p>TransformerFAM: Feedback attention is working memory​：</p>\\n<p>核心思想是利用反馈循环（feedback loop）来使网络能够关注其自身的潜在表示（latent representations）。</p>\\n<p>简单来说就是添加了两个记忆token用于存储之前的信息。所用的是attention和concat，也就是先将之前的记忆和当前的输入进行concat和layer norm再计算attention，最后加和。</p>\\n<h3>Mamba</h3>\\n<p>HiPPO: Recurrent Memory with Optimal Polynomial Projections</p>","autoDesc":true}');export{h as comp,d as data};
