import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as l,o,c,a as s,d as n,b as t,e as p}from"./app-SD3SAAIy.js";const i="/assets/images/rcmd/ch2.2/Snipaste_2024-07-16_21-38-18.png",u={},r=p('<h2 id="双塔召回模型" tabindex="-1"><a class="header-anchor" href="#双塔召回模型"><span>双塔召回模型</span></a></h2><p>双塔模型在推荐领域中是一个十分经典的模型，无论是在召回还是粗排阶段，都会是首选。这主要是得益于双塔模型结构，使得能够在线预估时满足低延时的要求。但也是因为其模型结构的问题，使得无法考虑到user和item特之间的特征交叉，使得影响模型最终效果，因此很多工作尝试调整经典双塔模型结构，在保持在线预估低延时的同时，保证双塔两侧之间有效的信息交叉。下面针对于经典双塔模型以及一些改进版本进行介绍。</p><h3 id="经典双塔模型" tabindex="-1"><a class="header-anchor" href="#经典双塔模型"><span>经典双塔模型</span></a></h3><p>DSSM(Deep Structured Semantic Model)是由微软研究院于CIKM在2013年提出的一篇工作，该模型主要用来解决NLP领域语义相似度任务 ，利用深度神经网络将文本表示为低维度的向量，用来提升搜索场景下文档和query匹配的问题。DSSM 模型的原理主要是：通过用户搜索行为中query 和 doc 的日志数据，通过深度学习网络将query和doc映射到到共同维度的语义空间中，通过最大化query和doc语义向量之 间的余弦相似度，从而训练得到隐含语义模型，即 query 侧特征的 embedding 和 doc 侧特征的 embedding，进而可以获取语句的低维 语义向量表达 sentence embedding，可以预测两句话的语义相似度。模型结构如下所示：</p><div align="center"><img src="https://pic4.zhimg.com/v2-7f75cc71f5e959d6efa95289d2f5ac13_r.jpg" style="zoom:45%;"></div><p>从上图可以看出，该网络结构比较简单，是一个由几层DNN组成网络，我们将要搜索文本(Query)和要匹配的文本(Document)的 embedding 输入到网络，网络输出为 128 维的向量，然后通过向量之间计算余弦相似度来计算向量之间距离，可以看作每一个 query 和 document 之间相似分数，然后在做 softmax。</p><p>而在推荐系统中，最为关键的问题是如何做好用户与item的匹配问题，因此对于推荐系统中DSSM模型的则是为 user 和 item 分别构建独立的子网络塔式结构，利用user和item的曝光或点击日期进行训练，最终得到user侧的embedding和item侧的embedding。因此在推荐系统中，常见的模型结构如下所示：</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220522103456450.png" style="zoom:60%;"></div><p>从模型结构上来看，主要包括两个部分：user侧塔和item侧塔，对于每个塔分别是一个DNN结构。通过两侧的特征输入，通过DNN模块到user和item的embedding，然后计算两者之间的相似度(常用內积或者余弦值，下面会说这两种方式的联系和区别)，因此对于user和item两侧最终得到的embedding维度需要保持一致，即最后一层全连接层隐藏单元个数相同。</p><p>在召回模型中，将这种检索行为视为多类分类问题，类似于YouTubeDNN模型。将物料库中所有的item视为一个类别，因此损失函数需要计算每个类的概率值：</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220522110742879.png" style="zoom:60%;"></div>',11),m=s("p",null,[n("其中"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"s"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{separator:"true"},","),s("mi",null,"y"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"s(x,y)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"mclose"},")")])])]),n("表示两个向量的相似度，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"P"),s("mo",{stretchy:"false"},"("),s("mi",null,"y"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"x"),s("mo",{separator:"true"},";"),s("mi",null,"θ"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"P(y|x;\\theta)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"mord"},"∣"),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mpunct"},";"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"θ"),s("span",{class:"mclose"},")")])])]),n("表示预测类别的概率，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"M")]),s("annotation",{encoding:"application/x-tex"},"M")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"M")])])]),n("表示物料库所有的item。但是在实际场景中，由于物料库中的item数量巨大，在计算上式时会十分的耗时，因此会采样一定的数量的负样本来近似计算，后面针对负样本的采样做一些简单介绍。")],-1),k=s("p",null,[n("以上就是推荐系统中经典的双塔模型，之所以在实际应用中非常常见，是因为"),s("strong",null,"在海量的候选数据进行召回的场景下，速度很快，效果说不上极端好，但一般而言效果也够用了"),n("。之所以双塔模型在服务时速度很快，是因为模型结构简单(两侧没有特征交叉)，但这也带来了问题，双塔的结构无法考虑两侧特征之间的交互信息，"),s("strong",null,"在一定程度上牺牲掉模型的部分精准性"),n("。例如在精排模型中，来自user侧和item侧的特征会在第一层NLP层就可以做细粒度的特征交互，而对于双塔模型，user侧和item侧的特征只会在最后的內积计算时发生，这就导致很多有用的信息在经过DNN结构时就已经被其他特征所模糊了，因此双塔结构由于其结构问题先天就会存在这样的问题。下面针对这个问题来看看一下现有模型的解决思路。")],-1),d=s("h3",{id:"senet双塔模型",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#senet双塔模型"},[s("span",null,"SENet双塔模型")])],-1),g=s("p",null,"SENet由Momenta在2017年提出，当时是一种应用于图像处理的新型网络结构。后来张俊林大佬将SENet引入了精排模型FiBiNET中，其作用是为了将大量长尾的低频特征抛弃，弱化不靠谱低频特征embedding的负面影响，强化高频特征的重要作用。那SENet结构到底是怎么样的呢，为什么可以起到特征筛选的作用？",-1),h=s("figure",null,[s("img",{src:i,alt:"",tabindex:"0",loading:"lazy"}),s("figcaption")],-1),v=s("p",null,"从上图可以看出SENET主要分为三个步骤Squeeze, Excitation, Re-weight：",-1),_=s("ul",null,[s("li",null,[s("p",null,"Squeeze阶段：我们对每个特征的Embedding向量进行数据压缩与信息汇总，即在Embedding维度计算均值："),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"z"),s("mi",null,"i")]),s("mo",null,"="),s("msub",null,[s("mi",null,"F"),s("mrow",null,[s("mi",null,"s"),s("mi",null,"q")])]),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"e"),s("mi",null,"i")]),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("mn",null,"1"),s("mi",null,"k")]),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"k")]),s("msubsup",null,[s("mi",null,"e"),s("mi",null,"i"),s("mrow",null,[s("mo",{stretchy:"false"},"("),s("mi",null,"t"),s("mo",{stretchy:"false"},")")])])]),s("annotation",{encoding:"application/x-tex"}," z_i = F_{sq}(e_i) = \\frac{1}{k} \\sum_{t=1}^k e_i^{(t)} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.04398em"}},"z"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.044em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"F"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.1032em","vertical-align":"-1.2671em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.686em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8361em"}},[s("span",{style:{top:"-1.8829em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03148em"}},"k")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.2671em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.0448em"}},[s("span",{style:{top:"-2.4231em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])]),s("span",{style:{top:"-3.2198em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mopen mtight"},"("),s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mclose mtight"},")")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2769em"}},[s("span")])])])])])])])])])]),s("p",null,"其中k表示Embedding的维度，Squeeze阶段是将每个特征的Squeeze转换成单一的数值。")]),s("li",null,[s("p",null,[n("Excitation阶段：这阶段是根据上一阶段得到的向量进行缩放，即将上阶段的得到的 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("mo",null,"×"),s("mi",null,"f")]),s("annotation",{encoding:"application/x-tex"},"1 \\times f")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f")])])]),n(" 的向量"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"Z")]),s("annotation",{encoding:"application/x-tex"},"Z")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z")])])]),n("先压缩成 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("mo",null,"×"),s("mfrac",null,[s("mi",null,"f"),s("mi",null,"r")])]),s("annotation",{encoding:"application/x-tex"},"1 \\times \\frac{f}{r}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.2772em","vertical-align":"-0.345em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9322em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"r")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.4461em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10764em"}},"f")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.345em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])]),n(" 长度，然后在放回到 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mn",null,"1"),s("mo",null,"×"),s("mi",null,"f")]),s("annotation",{encoding:"application/x-tex"},"1 \\times f")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f")])])]),n(" 的维度，其中"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r")]),s("annotation",{encoding:"application/x-tex"},"r")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r")])])]),n("表示压缩的程度。这个过程的具体操作就是经过两层DNN。")]),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"A"),s("mo",null,"="),s("msub",null,[s("mi",null,"F"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"x")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"Z"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("msub",null,[s("mi",null,"σ"),s("mn",null,"2")]),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"W"),s("mn",null,"2")]),s("msub",null,[s("mi",null,"σ"),s("mn",null,"1")]),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"W"),s("mn",null,"1")]),s("mi",null,"Z"),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," A = F_{ex}(Z) = \\sigma_2(W_2\\sigma_1(W_1Z)) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal"},"A"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"F"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"Z"),s("span",{class:"mclose"},"))")])])])])]),s("p",null,[n("该过程可以理解为：对于当前所有输入的特征，通过相互发生关联，来动态地判断哪些特征重要，哪些特征不重要，而这体现在Excitation阶段的输出结果 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"A")]),s("annotation",{encoding:"application/x-tex"},"A")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal"},"A")])])]),n("，其反应每个特征对应的重要性权重。")])]),s("li",null,[s("p",null,[n("Re-weight阶段：是将Excitation阶段得到的每个特征对应的权重 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"A")]),s("annotation",{encoding:"application/x-tex"},"A")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal"},"A")])])]),n(" 再乘回到特征对应的Embedding里，就完成了对特征重要性的加权操作。")]),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"V"),s("mo",null,"="),s("msub",null,[s("mi",null,"F"),s("mrow",null,[s("mi",null,"R"),s("mi",null,"e"),s("mi",null,"W"),s("mi",null,"e"),s("mi",null,"i"),s("mi",null,"g"),s("mi",null,"h"),s("mi",null,"t")])]),s("mo",{stretchy:"false"},"("),s("mi",null,"A"),s("mo",{separator:"true"},","),s("mi",null,"E"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mo",{stretchy:"false"},"["),s("msub",null,[s("mi",null,"a"),s("mn",null,"1")]),s("mo",null,"⋅"),s("msub",null,[s("mi",null,"e"),s("mn",null,"1")]),s("mo",{separator:"true"},","),s("mo",null,"⋯"),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"a"),s("mi",null,"f")]),s("mo",null,"⋅"),s("msub",null,[s("mi",null,"e"),s("mi",null,"f")]),s("mo",{stretchy:"false"},"]"),s("mo",null,"="),s("mo",{stretchy:"false"},"["),s("msub",null,[s("mi",null,"v"),s("mn",null,"1")]),s("mo",{separator:"true"},","),s("mo",null,"⋯"),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"v"),s("mi",null,"f")]),s("mo",{stretchy:"false"},"]")]),s("annotation",{encoding:"application/x-tex"}," V=F_{ReWeight }(A,E)=[a_1 \\cdot e_1,⋯,a_f \\cdot e_f]=[v_1,⋯,v_f] ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"F"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"g"),s("span",{class:"mord mathnormal mtight"},"h"),s("span",{class:"mord mathnormal mtight"},"t")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"A"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"E"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7306em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},"⋯"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"a"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10764em"}},"f")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"⋅"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10764em"}},"f")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mclose"},"]"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0361em","vertical-align":"-0.2861em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},"⋯"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10764em"}},"f")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mclose"},"]")])])])])])])],-1),b=s("p",null,"以上简单的介绍了一下SENet结构，可以发现这种结构可以通过对特征embedding先压缩，再交互，再选择，进而实现特征选择的效果。",-1),y={href:"https://zhuanlan.zhihu.com/p/358779957",target:"_blank",rel:"noopener noreferrer"},q=p('<div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220522152508824.png" style="zoom:70%;"></div><p>从上图可以发现，具体地是将双塔中的user塔和Item侧塔的特征输入部分加上一个SENet模块，通过SENet网络，动态地学习这些特征的重要性，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。</p><p>之所以SENet双塔模型是有效的呢？张俊林老师的解释是：双塔模型的问题在于User侧特征和Item侧特征交互太晚，在高层交互，会造成细节信息，也就是具体特征信息的损失，影响两侧特征交叉的效果。而SENet模块在最底层就进行了特征的过滤，使得很多无效低频特征即使被过滤掉，这样更多有用的信息被保留到了双塔的最高层，使得两侧的交叉效果很好；同时由于SENet模块选择出更加重要的信息，使得User侧和Item侧特征之间的交互表达方面增强了DNN双塔的能力。</p><p>因此SENet双塔模型主要是从特征选择的角度，提高了两侧特征交叉的有效性，减少了噪音对有效信息的干扰，进而提高了双塔模型的效果。此外，除了这样的方式，还可以通过增加通道的方式来增强两侧的信息交互。即对于user和item两侧不仅仅使用一个DNN结构，而是可以通过不同结构(如FM，DCN等)来建模user和item的自身特征交叉，例如下图所示：</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/v2-9c2f7a30c6cadc47be23d6797f095b61_b.jpg" style="zoom:80%;"></div><p>这样对于user和item侧会得到多个embedding，类似于多兴趣的概念。通过得到的多个user和item的embedding，然后分别计算余弦值再相加(两侧的Embedding维度需要对齐)，进而增加了双塔两侧的信息交互。而这种方法在腾讯进行过尝试，他们提出的“并联”双塔就是按照这样的思路，感兴趣的可以了解一下。</p><h3 id="多目标的双塔模型" tabindex="-1"><a class="header-anchor" href="#多目标的双塔模型"><span>多目标的双塔模型</span></a></h3><p>现如今多任务学习在实际的应用场景也十分的常见，主要是因为实际场景中业务复杂，往往有很多的衡量指标，例如点击，评论，收藏，关注，转发等。在多任务学习中，往往会针对不同的任务使用一个独有的tower，然后优化不同任务损失。那么针对双塔模型应该如何构建多任务学习框架呢？</p><div align="center"><img src="https://cdn.jsdelivr.net/gh/swallown1/blogimages@main/images/image-20220523113206177.png" style="zoom:60%;"></div>',9),f=s("p",null,"这种模型结构，可以针对多目标进行联合建模，通过多任务学习的结构，一方面可以利用不同任务之间的信息共享，为一些稀疏特征提供其他任务中的迁移信息，另一方面可以在召回时，直接使用一个模型得到多个目标预测，解决了多个模型维护困难的问题。也就是说，在线上通过这一个模型就可以同时得到多个指标，例如视频场景，一个模型就可以直接得到点赞，品论，转发等目标的预测值，进而通过这些值计算分数获得最终的Top-K召回结果。",-1),x=s("h2",{id:"双塔模型的细节",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#双塔模型的细节"},[s("span",null,"双塔模型的细节")])],-1),w=s("p",null,"关于双塔模型，其模型结构相比排序模型来说很简单，没有过于复杂的结构。但除了结构，有一些细节部分容易被忽视，而这些细节部分往往比模型结构更加重要，因此下面主要介绍一下双塔模型中需要主要的一些细节问题。",-1),z=s("h3",{id:"归一化与温度系数",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#归一化与温度系数"},[s("span",null,"归一化与温度系数")])],-1),M={href:"https://dl.acm.org/doi/pdf/10.1145/3298689.3346996",target:"_blank",rel:"noopener noreferrer"},N=s("ul",null,[s("li",null,[s("p",null,"归一化：对user侧和item侧的输入embedding，进行L2归一化"),s("p",null,"​ $$u(x,\\theta) \\leftarrow = \\frac{u(x,\\theta)}{||u(x,\\theta)||_2}$$"),s("p",null,"​ $$v(x,\\theta) \\leftarrow = \\frac{v(x,\\theta)}{||v(x,\\theta)||_2}$$")]),s("li",null,[s("p",null,[n("温度系数：在归一化之后的向量计算內积之后，除以一个固定的超参 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"r")]),s("annotation",{encoding:"application/x-tex"},"r")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r")])])]),n(" ，论文中命名为温度系数。")]),s("p",null,"​ $$s(u,v) = \\frac{<u(x,\\theta), v(x,\\theta)>}{r}$$"),s("p",null,"那为什么需要进行上述的两个操作呢？")]),s("li",null,[s("p",null,"归一化的操作主要原因是因为向量点积距离是非度量空间，不满足三角不等式，而归一化的操作使得点击行为转化成了欧式距离。"),s("p",null,[n("首先向量点积是向量对应位相乘并求和，即向量內积。而向量內积"),s("strong",null,"不保序"),n("，例如空间上三个点(A=(10,0),B=(0,10),C=(11,0))，利用向量点积计算的距离 dis(A,B) < dis(A,C)，但是在欧式距离下这是错误的。而归一化的操作则会让向量点积转化为欧式距离，例如 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"u"),s("mi",null,"s"),s("mi",null,"e"),s("msub",null,[s("mi",null,"r"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])])]),s("annotation",{encoding:"application/x-tex"},"user_{emb}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"se"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0278em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),n(" 表示归一化user的embedding， "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"i"),s("mi",null,"t"),s("mi",null,"e"),s("msub",null,[s("mi",null,"m"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])])]),s("annotation",{encoding:"application/x-tex"},"item_{emb}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8095em","vertical-align":"-0.15em"}}),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),n(" 表示归一化 item 的embedding，那么两者之间的欧式距离 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"∣"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"u"),s("mi",null,"s"),s("mi",null,"e"),s("msub",null,[s("mi",null,"r"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mo",null,"−"),s("mi",null,"i"),s("mi",null,"t"),s("mi",null,"e"),s("msub",null,[s("mi",null,"m"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mi",{mathvariant:"normal"},"∣"),s("mi",{mathvariant:"normal"},"∣")]),s("annotation",{encoding:"application/x-tex"},"||user_{emb} - item_{emb}||")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"∣∣"),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"se"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0278em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣∣")])])]),n(" 如下， 可以看出归一化的向量点积已转化成了欧式距离。")]),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"∣"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"u"),s("mi",null,"s"),s("mi",null,"e"),s("msub",null,[s("mi",null,"r"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mo",null,"−"),s("mi",null,"i"),s("mi",null,"t"),s("mi",null,"e"),s("msub",null,[s("mi",null,"m"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mi",{mathvariant:"normal"},"∣"),s("mi",{mathvariant:"normal"},"∣"),s("mo",null,"="),s("msqrt",null,[s("mrow",null,[s("mi",{mathvariant:"normal"},"∣"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"u"),s("mi",null,"s"),s("mi",null,"e"),s("msub",null,[s("mi",null,"r"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mi",{mathvariant:"normal"},"∣"),s("msup",null,[s("mi",{mathvariant:"normal"},"∣"),s("mn",null,"2")]),s("mo",null,"+"),s("mi",{mathvariant:"normal"},"∣"),s("mi",{mathvariant:"normal"},"∣"),s("mi",null,"i"),s("mi",null,"t"),s("mi",null,"e"),s("msub",null,[s("mi",null,"m"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mi",{mathvariant:"normal"},"∣"),s("msup",null,[s("mi",{mathvariant:"normal"},"∣"),s("mn",null,"2")]),s("mo",null,"−"),s("mn",null,"2"),s("mo",null,"<"),s("mi",null,"u"),s("mi",null,"s"),s("mi",null,"e"),s("msub",null,[s("mi",null,"r"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mo",{separator:"true"},","),s("mi",null,"i"),s("mi",null,"t"),s("mi",null,"e"),s("msub",null,[s("mi",null,"m"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mo",null,">")])]),s("mo",null,"="),s("msqrt",null,[s("mrow",null,[s("mn",null,"2"),s("mo",null,"−"),s("mn",null,"2"),s("mo",null,"<"),s("mi",null,"u"),s("mi",null,"s"),s("mi",null,"e"),s("msub",null,[s("mi",null,"r"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mo",{separator:"true"},","),s("mi",null,"i"),s("mi",null,"t"),s("mi",null,"e"),s("msub",null,[s("mi",null,"m"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"m"),s("mi",null,"b")])]),s("mo",null,">")])])]),s("annotation",{encoding:"application/x-tex"}," ||user_{emb} - item_{emb}||=\\sqrt{||user_{emb}||^2+||item_{emb}||^2-2<user_{emb},item_{emb}>} = \\sqrt{2-2<user_{emb},item_{emb}>} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"∣∣"),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"se"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0278em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣∣"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.24em","vertical-align":"-0.2561em"}}),s("span",{class:"mord sqrt"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9839em"}},[s("span",{class:"svg-align",style:{top:"-3.2em"}},[s("span",{class:"pstrut",style:{height:"3.2em"}}),s("span",{class:"mord",style:{"padding-left":"1em"}},[s("span",{class:"mord"},"∣∣"),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"se"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0278em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord"},"∣"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7401em"}},[s("span",{style:{top:"-2.989em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"∣∣"),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord"},"∣"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7401em"}},[s("span",{style:{top:"-2.989em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"2"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"<"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"se"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0278em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},">")])]),s("span",{style:{top:"-2.9439em"}},[s("span",{class:"pstrut",style:{height:"3.2em"}}),s("span",{class:"hide-tail",style:{"min-width":"1.02em",height:"1.28em"}},[s("svg",{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.28em",viewBox:"0 0 400000 1296",preserveAspectRatio:"xMinYMin slice"},[s("path",{d:`M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z`})])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2561em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.24em","vertical-align":"-0.2736em"}}),s("span",{class:"mord sqrt"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.9664em"}},[s("span",{class:"svg-align",style:{top:"-3.2em"}},[s("span",{class:"pstrut",style:{height:"3.2em"}}),s("span",{class:"mord",style:{"padding-left":"1em"}},[s("span",{class:"mord"},"2"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"2"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"<"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"se"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"r"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0278em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"m"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"mb")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},">")])]),s("span",{style:{top:"-2.9264em"}},[s("span",{class:"pstrut",style:{height:"3.2em"}}),s("span",{class:"hide-tail",style:{"min-width":"1.02em",height:"1.28em"}},[s("svg",{xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.28em",viewBox:"0 0 400000 1296",preserveAspectRatio:"xMinYMin slice"},[s("path",{d:`M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z`})])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2736em"}},[s("span")])])])])])])])])]),s("p",null,"那没啥非要转为欧式距离呢？这是因为ANN一般是通过计算欧式距离进行检索，这样转化成欧式空间，保证训练和检索一致。")])],-1),S=p('<h3 id="模型的应用" tabindex="-1"><a class="header-anchor" href="#模型的应用"><span>模型的应用</span></a></h3><p>在实际的工业应用场景中，分为离线训练和在线服务两个环节。</p><ul><li>在离线训练阶段，同过训练数据，训练好模型参数。然后将候选库中所有的item集合离线计算得到对应的embedding，并存储进ANN检索系统，比如faiss。为什么将离线计算item集合，主要是因为item的会相对稳定，不会频繁的变动，而对于用户而言，如果将用户行为作为user侧的输入，那么user的embedding会随着用户行为的发生而不断变化，因此对于user侧的embedding需要实时的计算。</li><li>在线服务阶段，正是因为用户的行为变化需要被即使的反应在用户的embedding中，以更快的反应用户当前的兴趣，即可以实时地体现用户即时兴趣的变化。因此在线服务阶段需要实时的通过拼接用户特征，输入到user侧的DNN当中，进而得到user embedding，在通过user embedding去 faiss中进行ANN检索，召回最相似的K个item embedding。</li></ul><p>可以看到双塔模型结构十分的适合实际的应用场景，在快速服务的同时，还可以更快的反应用户即时兴趣的变化。</p><h3 id="负样本采样" tabindex="-1"><a class="header-anchor" href="#负样本采样"><span>负样本采样</span></a></h3><p>相比于排序模型而言，召回阶段的模型除了在结构上的不同，在样本选择方面也存在着很大的差异，可以说样本的选择很大程度上会影响召回模型的效果。对于召回模型而言，其负样本并不能和排序模型一样只使用展现未点击样本，因为召回模型在线上面临的数据分布是全部的item，而不仅仅是展现未点击样本。因此在离线训练时，需要让其保证和线上分布尽可能一致，所以在负样本的选择样要尽可能的增加很多未被曝光的item。下面简单的介绍一些常见的采样方法：</p><h4 id="全局随机采样" tabindex="-1"><a class="header-anchor" href="#全局随机采样"><span>全局随机采样</span></a></h4><p>全局随机采样指：从全局候选item里面随机抽取一定数量item做为召回模型的负样本。这样的方式实现简单，也可以让模型尽可能的和线上保持一致的分布，尽可能的多的让模型对于全局item有区分的能力。例如YoutubeDNN算法。</p><p>但这样的方式也会存在一定的问题，由于候选的item属于长尾数据，即“八二定律”，也就是说少数热门物料占据了绝大多数的曝光与点击。因此存随机的方式只能让模型在学到粗粒度上差异，对一些尾部item并不友好。</p><h4 id="全局随机采样-热门打压" tabindex="-1"><a class="header-anchor" href="#全局随机采样-热门打压"><span>全局随机采样 + 热门打压</span></a></h4><p>针对于全局随机采样的不足，一个直观的方法是针对于item的热度item进行打压，即对于热门的item很多用户可能会点击，需要进行一定程度的欠采样，使得模型更加关注一些非热门的item。 此外在进行负样本采样时，应该对一些热门item进行适当的过采样，这可以尽可能的让模型对于负样本有更加细粒度的区分。例如在word2vec中，负采样方法是根据word的频率，对 negative words进行随机抽样，降 低 negative words 量级。</p><p>之所以热门item做负样本时，要适当过采样，增加负样本难度。因为对于全量的item，模型可以轻易的区分一些和用户兴趣差异性很大的item，难点在于很难区分一些和用户兴趣相似的item。因此在训练模型时，需要适当的增加一些难以区分的负样本来提升模型面对相似item的分区能力。</p><h4 id="hard-negative增强样本" tabindex="-1"><a class="header-anchor" href="#hard-negative增强样本"><span>Hard Negative增强样本</span></a></h4><p>Hard Negative指的是选取一部分匹配度适中的item，能够增加模型在训练时的难度，提升模型能学习到item之间细粒度上的差异。至于 如何选取在工业界也有很多的解决方案。</p>',14),D={href:"https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb",target:"_blank",rel:"noopener noreferrer"},E={href:"http://research.baidu.com/Public/uploads/5d12eca098d40.pdf",target:"_blank",rel:"noopener noreferrer"},F={href:"https://arxiv.org/pdf/2006.11632.pdf",target:"_blank",rel:"noopener noreferrer"},L=p(`<h4 id="batch内随机选择负采样" tabindex="-1"><a class="header-anchor" href="#batch内随机选择负采样"><span>Batch内随机选择负采样</span></a></h4><p>基于batch的负采样方法是将batch内选择除了正样本之外的其它Item，做为负样本，其本质就是利用其他样本的正样本随机采样作为自己的负样本。这样的方法可以作为负样本的选择方式，特别是在如今分布式训练以及增量训练的场景中是一个非常值得一试的方法。但这种方法也存在他的问题，基于batch的负采样方法受batch的影响很大，当batch的分布与整体的分布差异很大时就会出现问题，同时batch内负采样也会受到热门item的影响，需要考虑打压热门item的问题。至于解决的办法，Google的双塔召回模型中给出了答案，想了解的同学可以去学习一下。</p><p>总的来说负样本的采样方法，不光是双塔模型应该重视的工作，而是所有召回模型都应该仔细考虑的方法。</p><h2 id="代码实现" tabindex="-1"><a class="header-anchor" href="#代码实现"><span>代码实现</span></a></h2><p>下面使用一点资讯提供的数据，实践一下DSSM召回模型。该模型的实现主要参考：DeepCtr和DeepMatch模块。</p><h3 id="模型训练数据" tabindex="-1"><a class="header-anchor" href="#模型训练数据"><span>模型训练数据</span></a></h3><p>1、数据预处理 用户侧主要包含一些用户画像属性（用户性别，年龄，所在省市，使用设备及系统）；新闻侧主要包括新闻的创建时间，题目，所属 一级、二级类别，题片个数以及关键词。下面主要是对着两部分数据的简单处理：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">proccess</span><span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token keyword">if</span> <span class="token builtin">file</span><span class="token operator">==</span><span class="token string">&quot;user_info_data_5w.csv&quot;</span><span class="token punctuation">:</span>
     data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token builtin">file</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">&quot;\\t&quot;</span><span class="token punctuation">,</span>index_col<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;age&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;age&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> get_pro_age<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;gender&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;gender&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> get_pro_age<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>

     data<span class="token punctuation">[</span><span class="token string">&quot;province&quot;</span><span class="token punctuation">]</span><span class="token operator">=</span>data<span class="token punctuation">[</span><span class="token string">&quot;province&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">&#39;ffill&#39;</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;city&quot;</span><span class="token punctuation">]</span><span class="token operator">=</span>data<span class="token punctuation">[</span><span class="token string">&quot;city&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">&#39;ffill&#39;</span><span class="token punctuation">)</span>

     data<span class="token punctuation">[</span><span class="token string">&quot;device&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;device&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">&#39;ffill&#39;</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;os&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;os&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>method<span class="token operator">=</span><span class="token string">&#39;ffill&#39;</span><span class="token punctuation">)</span>
     <span class="token keyword">return</span> data

 <span class="token keyword">elif</span> <span class="token builtin">file</span><span class="token operator">==</span><span class="token string">&quot;doc_info.txt&quot;</span><span class="token punctuation">:</span>
     data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token builtin">file</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">&quot;\\t&quot;</span><span class="token punctuation">)</span>
     data<span class="token punctuation">.</span>columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;title&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;ctime&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;img_num&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;cate&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;key_words&quot;</span><span class="token punctuation">]</span>
     select_column <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;title_len&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;ctime&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;img_num&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;cate&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;key_words&quot;</span><span class="token punctuation">]</span>

     <span class="token comment"># 去除时间为nan的新闻以及除脏数据</span>
     data<span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&quot;ctime&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>notna<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&quot;ctime&quot;</span><span class="token punctuation">]</span> <span class="token operator">!=</span> <span class="token string">&#39;Android&#39;</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
     data<span class="token punctuation">[</span><span class="token string">&#39;ctime&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&#39;ctime&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;str&#39;</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&#39;ctime&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&#39;ctime&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&#39;ctime&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> pd<span class="token punctuation">.</span>to_datetime<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&#39;ctime&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> unit<span class="token operator">=</span><span class="token string">&#39;s&#39;</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">&#39;coerce&#39;</span><span class="token punctuation">)</span>


     <span class="token comment"># 这里存在nan字符串和异常数据</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> pro_sub_cate<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;img_num&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;img_num&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;img_num&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;img_num&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>photoNums<span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;title_len&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">0</span><span class="token punctuation">)</span>
     data<span class="token punctuation">[</span><span class="token string">&quot;cate&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&quot;cate&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>fillna<span class="token punctuation">(</span><span class="token string">&#39;其他&#39;</span><span class="token punctuation">)</span>

     <span class="token keyword">return</span> data<span class="token punctuation">[</span>select_column<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、构造训练样本 该部分主要是根据用户的交互日志中前6天的数据作为训练集，第7天的数据作为测试集，来构造模型的训练测试样本。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">dealsample</span><span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">,</span> doc_data<span class="token punctuation">,</span> user_data<span class="token punctuation">,</span> s_data_str <span class="token operator">=</span> <span class="token string">&quot;2021-06-24 00:00:00&quot;</span><span class="token punctuation">,</span> e_data_str<span class="token operator">=</span><span class="token string">&quot;2021-06-30 23:59:59&quot;</span><span class="token punctuation">,</span> neg_num<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token comment"># 先处理时间问题</span>
 data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token builtin">file</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">&quot;\\t&quot;</span><span class="token punctuation">,</span>index_col<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
 data<span class="token punctuation">[</span><span class="token string">&#39;expo_time&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&#39;expo_time&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;str&#39;</span><span class="token punctuation">)</span>
 data<span class="token punctuation">[</span><span class="token string">&#39;expo_time&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">&#39;expo_time&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 data<span class="token punctuation">[</span><span class="token string">&#39;expo_time&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> pd<span class="token punctuation">.</span>to_datetime<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&#39;expo_time&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> unit<span class="token operator">=</span><span class="token string">&#39;s&#39;</span><span class="token punctuation">,</span> errors<span class="token operator">=</span><span class="token string">&#39;coerce&#39;</span><span class="token punctuation">)</span>

 s_date <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>strptime<span class="token punctuation">(</span>s_data_str<span class="token punctuation">,</span><span class="token string">&quot;%Y-%m-%d %H:%M:%S&quot;</span><span class="token punctuation">)</span>
 e_date <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>strptime<span class="token punctuation">(</span>e_data_str<span class="token punctuation">,</span><span class="token string">&quot;%Y-%m-%d %H:%M:%S&quot;</span><span class="token punctuation">)</span> <span class="token operator">+</span> datetime<span class="token punctuation">.</span>timedelta<span class="token punctuation">(</span>days<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
 t_date <span class="token operator">=</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>strptime<span class="token punctuation">(</span>e_data_str<span class="token punctuation">,</span><span class="token string">&quot;%Y-%m-%d %H:%M:%S&quot;</span><span class="token punctuation">)</span>

 <span class="token comment"># 选取训练和测试所需的数据</span>
 all_data_tmp <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token operator">&gt;=</span>s_date<span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span>t_date<span class="token punctuation">)</span><span class="token punctuation">]</span>

 <span class="token comment"># 处理训练数据集  防止穿越样本</span>
 <span class="token comment"># 1. merge 新闻信息，得到曝光时间和新闻创建时间； inner join 去除doc_data之外的新闻</span>
 all_data_tmp <span class="token operator">=</span> all_data_tmp<span class="token punctuation">.</span>join<span class="token punctuation">(</span>doc_data<span class="token punctuation">.</span>set_index<span class="token punctuation">(</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>on<span class="token operator">=</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span>how<span class="token operator">=</span><span class="token string">&#39;inner&#39;</span><span class="token punctuation">)</span>

 <span class="token comment"># 发现还存在 ctime大于expo_time的交互存在  去除这部分错误数据</span>
 all_data_tmp <span class="token operator">=</span> all_data_tmp<span class="token punctuation">[</span><span class="token punctuation">(</span>all_data_tmp<span class="token punctuation">[</span><span class="token string">&quot;ctime&quot;</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span>all_data_tmp<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

 <span class="token comment"># 2. 去除与新闻的创建时间在测试数据时间内的交互  ()</span>
 train_data <span class="token operator">=</span> all_data_tmp<span class="token punctuation">[</span><span class="token punctuation">(</span>all_data_tmp<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token operator">&gt;=</span>s_date<span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>all_data_tmp<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span>e_date<span class="token punctuation">)</span><span class="token punctuation">]</span>
 train_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span><span class="token punctuation">(</span>train_data<span class="token punctuation">[</span><span class="token string">&quot;ctime&quot;</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span>e_date<span class="token punctuation">)</span><span class="token punctuation">]</span>

 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;有效的样本数：&quot;</span><span class="token punctuation">,</span>train_data<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 <span class="token comment"># 负采样</span>
 <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&quot;neg_sample.pkl&quot;</span><span class="token punctuation">)</span> <span class="token keyword">and</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>getsize<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&quot;neg_sample.pkl&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     neg_samples <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_pickle<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&quot;neg_sample.pkl&quot;</span><span class="token punctuation">)</span>
     <span class="token comment"># train_neg_samples.insert(loc=2, column=&quot;click&quot;, value=[0] * train_neg_samples[&quot;user_id&quot;].count())</span>
 <span class="token keyword">else</span><span class="token punctuation">:</span>
     <span class="token comment"># 进行负采样的时候对于样本进行限制，只对一定时间范围之内的样本进行负采样</span>
     doc_data_tmp <span class="token operator">=</span> doc_data<span class="token punctuation">[</span><span class="token punctuation">(</span>doc_data<span class="token punctuation">[</span><span class="token string">&quot;ctime&quot;</span><span class="token punctuation">]</span><span class="token operator">&gt;=</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>strptime<span class="token punctuation">(</span><span class="token string">&quot;2021-06-01 00:00:00&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;%Y-%m-%d %H:%M:%S&quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
     neg_samples <span class="token operator">=</span> negSample_like_word2vec<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> doc_data_tmp<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> user_data<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> neg_num<span class="token operator">=</span>neg_num<span class="token punctuation">)</span>
     neg_samples <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>neg_samples<span class="token punctuation">,</span> columns<span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
     neg_samples<span class="token punctuation">.</span>to_pickle<span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&quot;neg_sample.pkl&quot;</span><span class="token punctuation">)</span>

 train_pos_samples <span class="token operator">=</span> train_data<span class="token punctuation">[</span>train_data<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token comment"># 取正样本</span>

 neg_samples_df <span class="token operator">=</span> train_data<span class="token punctuation">[</span>train_data<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
 train_neg_samples <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>neg_samples_df<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>n<span class="token operator">=</span>train_pos_samples<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">,</span>neg_samples<span class="token punctuation">]</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 取负样本</span>

 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;训练集正样本数：&quot;</span><span class="token punctuation">,</span>train_pos_samples<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;训练集负样本数：&quot;</span><span class="token punctuation">,</span>train_neg_samples<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 train_data_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>train_neg_samples<span class="token punctuation">,</span>train_pos_samples<span class="token punctuation">]</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
 train_data_df <span class="token operator">=</span> train_data_df<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>frac<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># shuffle</span>

 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;训练集总样本数：&quot;</span><span class="token punctuation">,</span>train_data_df<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 test_data_df <span class="token operator">=</span>  all_data_tmp<span class="token punctuation">[</span><span class="token punctuation">(</span>all_data_tmp<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token operator">&gt;</span>e_date<span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>all_data_tmp<span class="token punctuation">[</span><span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span>t_date<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;expo_time&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;测试集总样本数：&quot;</span><span class="token punctuation">,</span>test_data_df<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;测试集总样本数：&quot;</span><span class="token punctuation">,</span>test_data_df<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 all_data_df <span class="token operator">=</span>  pd<span class="token punctuation">.</span>concat<span class="token punctuation">(</span><span class="token punctuation">[</span>train_data_df<span class="token punctuation">,</span> test_data_df<span class="token punctuation">]</span><span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;总样本数：&quot;</span><span class="token punctuation">,</span>all_data_df<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 <span class="token keyword">return</span> all_data_df
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3、负样本采样 该部分主要采用基于item的展现次数对全局item进行负采样。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">negSample_like_word2vec</span><span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> all_items<span class="token punctuation">,</span> all_users<span class="token punctuation">,</span> neg_num<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token triple-quoted-string string">&quot;&quot;&quot;
 为所有item计算一个采样概率，根据概率为每个用户采样neg_num个负样本，返回所有负样本对
 1. 统计所有item在交互中的出现频次
 2. 根据频次进行排序，并计算item采样概率（频次出现越多，采样概率越低，打压热门item）
 3. 根据采样概率，利用多线程为每个用户采样 neg_num 个负样本
 &quot;&quot;&quot;</span>
 pos_samples <span class="token operator">=</span> train_data<span class="token punctuation">[</span>train_data<span class="token punctuation">[</span><span class="token string">&quot;click&quot;</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

 pos_samples_dic <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
 <span class="token keyword">for</span> idx<span class="token punctuation">,</span>u <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>pos_samples<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     pos_list <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>pos_samples<span class="token punctuation">[</span>pos_samples<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span> <span class="token operator">==</span> u<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
     <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>pos_list<span class="token punctuation">)</span> <span class="token operator">&gt;=</span> <span class="token number">30</span><span class="token punctuation">:</span>  <span class="token comment"># 30是拍的  需要数据统计的支持确定</span>
         pos_samples_dic<span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> pos_list<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
     <span class="token keyword">else</span><span class="token punctuation">:</span>
         pos_samples_dic<span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> pos_list

 <span class="token comment"># 统计出现频次</span>
 article_counts <span class="token operator">=</span> train_data<span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span>
 df_article_counts <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>article_counts<span class="token punctuation">)</span>
 dic_article_counts <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>df_article_counts<span class="token punctuation">.</span>index<span class="token punctuation">.</span>values<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>df_article_counts<span class="token punctuation">.</span>article_id<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 <span class="token keyword">for</span> item <span class="token keyword">in</span> all_items<span class="token punctuation">:</span>
     <span class="token keyword">if</span> item<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">not</span> <span class="token keyword">in</span> dic_article_counts<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
         dic_article_counts<span class="token punctuation">[</span>item<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

 <span class="token comment"># 根据频次排序, 并计算每个item的采样概率</span>
 tmp <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>dic_article_counts<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span>x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 降序</span>
 n_articles <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>
 article_prob <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
 <span class="token keyword">for</span> idx<span class="token punctuation">,</span> item <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tmp<span class="token punctuation">)</span><span class="token punctuation">:</span>
     article_prob<span class="token punctuation">[</span>item<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> cal_pos<span class="token punctuation">(</span>idx<span class="token punctuation">,</span> n_articles<span class="token punctuation">)</span>

 <span class="token comment"># 为每个用户进行负采样</span>
 article_id_list <span class="token operator">=</span> <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> a <span class="token keyword">in</span> article_prob<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
 article_pro_list <span class="token operator">=</span> <span class="token punctuation">[</span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> a <span class="token keyword">in</span> article_prob<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
 pos_sample_users <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>pos_samples_dic<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 all_users_list <span class="token operator">=</span> <span class="token punctuation">[</span>u<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> u <span class="token keyword">in</span> all_users<span class="token punctuation">]</span>

 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;start negative sampling !!!!!!&quot;</span><span class="token punctuation">)</span>
 pool <span class="token operator">=</span> multiprocessing<span class="token punctuation">.</span>Pool<span class="token punctuation">(</span>core_size<span class="token punctuation">)</span>
 res <span class="token operator">=</span> pool<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>SampleOneProb<span class="token punctuation">(</span><span class="token punctuation">(</span>pos_sample_users<span class="token punctuation">,</span>article_id_list<span class="token punctuation">,</span>article_pro_list<span class="token punctuation">,</span>pos_samples_dic<span class="token punctuation">,</span>neg_num<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tqdm<span class="token punctuation">(</span>all_users_list<span class="token punctuation">)</span><span class="token punctuation">)</span>
 pool<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
 pool<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span>

 neg_sample_dic <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
 <span class="token keyword">for</span> idx<span class="token punctuation">,</span> u <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token builtin">enumerate</span><span class="token punctuation">(</span>all_users_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     neg_sample_dic<span class="token punctuation">[</span>u<span class="token punctuation">]</span> <span class="token operator">=</span> res<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>

 <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>k<span class="token punctuation">,</span>i<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span>v <span class="token keyword">in</span> neg_sample_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> v<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="dssm-模型" tabindex="-1"><a class="header-anchor" href="#dssm-模型"><span>DSSM 模型</span></a></h3><p>1、模型构建</p><p>​ 模型构建部分主要是将输入的user 特征以及 item 特征处理完之后分别送入两侧的DNN结构。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">DSSM</span><span class="token punctuation">(</span>user_feature_columns<span class="token punctuation">,</span> item_feature_columns<span class="token punctuation">,</span> dnn_units<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
        temp<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> task<span class="token operator">=</span><span class="token string">&#39;binary&#39;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 构建所有特征的Input层和Embedding层</span>
    feature_encode <span class="token operator">=</span> FeatureEncoder<span class="token punctuation">(</span>user_feature_columns <span class="token operator">+</span> item_feature_columns<span class="token punctuation">)</span>
    feature_input_layers_list <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>feature_encode<span class="token punctuation">.</span>feature_input_layer_dict<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 特征处理</span>
    user_dnn_input<span class="token punctuation">,</span> item_dnn_input <span class="token operator">=</span> process_feature<span class="token punctuation">(</span>user_feature_columns<span class="token punctuation">,</span>\\
        item_feature_columns<span class="token punctuation">,</span> feature_encode<span class="token punctuation">)</span>

    <span class="token comment"># 构建模型的核心层</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>user_dnn_input<span class="token punctuation">)</span> <span class="token operator">&gt;=</span> <span class="token number">2</span><span class="token punctuation">:</span>
        user_dnn_input <span class="token operator">=</span> Concatenate<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>user_dnn_input<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        user_dnn_input <span class="token operator">=</span> user_dnn_input<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>item_dnn_input<span class="token punctuation">)</span> <span class="token operator">&gt;=</span> <span class="token number">2</span><span class="token punctuation">:</span>
        item_dnn_input <span class="token operator">=</span> Concatenate<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>item_dnn_input<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        item_dnn_input <span class="token operator">=</span> item_dnn_input<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    user_dnn_input <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>user_dnn_input<span class="token punctuation">)</span> 
    item_dnn_input <span class="token operator">=</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>item_dnn_input<span class="token punctuation">)</span>
    user_dnn_out <span class="token operator">=</span> DNN<span class="token punctuation">(</span>dnn_units<span class="token punctuation">)</span><span class="token punctuation">(</span>user_dnn_input<span class="token punctuation">)</span>
    item_dnn_out <span class="token operator">=</span> DNN<span class="token punctuation">(</span>dnn_units<span class="token punctuation">)</span><span class="token punctuation">(</span>item_dnn_input<span class="token punctuation">)</span>


    <span class="token comment"># 计算相似度</span>
    scores <span class="token operator">=</span> CosinSimilarity<span class="token punctuation">(</span>temp<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>user_dnn_out<span class="token punctuation">,</span> item_dnn_out<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># (B,1)</span>
    <span class="token comment"># 确定拟合目标</span>
    output <span class="token operator">=</span> PredictLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span>
    <span class="token comment"># 根据输入输出构建模型</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>feature_input_layers_list<span class="token punctuation">,</span> output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、CosinSimilarity相似度计算</p><p>​ 在余弦相似度计算，主要是注意使用归一化以及温度系数的技巧。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;inputs 是一个列表&quot;&quot;&quot;</span>
    query<span class="token punctuation">,</span> candidate <span class="token operator">=</span> inputs 
    <span class="token comment"># 计算两个向量的二范数</span>
    query_norm <span class="token operator">=</span> tf<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>query<span class="token punctuation">,</span> axis<span class="token operator">=</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">)</span> <span class="token comment"># (B, 1)</span>
    candidate_norm <span class="token operator">=</span> tf<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>candidate<span class="token punctuation">,</span> axis<span class="token operator">=</span>self<span class="token punctuation">.</span>axis<span class="token punctuation">)</span>
    <span class="token comment"># 计算向量点击，即內积操作</span>
    scores <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_sum<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>query<span class="token punctuation">,</span> candidate<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#(B,1)</span>
    <span class="token comment"># 相似度除以二范数, 防止除零</span>
    scores <span class="token operator">=</span> tf<span class="token punctuation">.</span>divide<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> query_norm <span class="token operator">*</span> candidate_norm <span class="token operator">+</span> <span class="token number">1e-8</span><span class="token punctuation">)</span>
    <span class="token comment"># 对score的范围限制到(-1, 1)之间</span>
    scores <span class="token operator">=</span> tf<span class="token punctuation">.</span>clip_by_value<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># 乘以温度系数</span>
    score <span class="token operator">=</span> scores <span class="token operator">*</span> self<span class="token punctuation">.</span>temperature 
    <span class="token keyword">return</span> score 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="模型训练" tabindex="-1"><a class="header-anchor" href="#模型训练"><span>模型训练</span></a></h3><p>1、稀疏特征编码 该部分主要是针对于用户侧和新闻侧的稀疏特征进行编码，并将训练样本join上两侧的特征。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code> <span class="token comment"># 数据和测试数据</span>
 data<span class="token punctuation">,</span> user_data<span class="token punctuation">,</span> doc_data <span class="token operator">=</span> get_all_data<span class="token punctuation">(</span><span class="token punctuation">)</span>

 <span class="token comment"># 1.Label Encoding for sparse features,and process sequence features with \`gen_date_set\` and \`gen_model_input\`</span>
 feature_max_idx <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
 feature_encoder <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

 user_sparse_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;device&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;os&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;province&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;city&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;age&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;gender&quot;</span><span class="token punctuation">]</span>
 <span class="token keyword">for</span> feature <span class="token keyword">in</span> user_sparse_features<span class="token punctuation">:</span>
     lbe <span class="token operator">=</span> LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
     user_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> lbe<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>user_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
     feature_max_idx<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> user_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
     feature_encoder<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> lbe


 doc_sparse_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;cate&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">]</span>
 doc_dense_features <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&quot;title_len&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;img_num&quot;</span><span class="token punctuation">]</span>

 <span class="token keyword">for</span> feature <span class="token keyword">in</span> doc_sparse_features<span class="token punctuation">:</span>
     lbe <span class="token operator">=</span> LabelEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
     <span class="token keyword">if</span> feature <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">&quot;cate&quot;</span><span class="token punctuation">,</span><span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
         <span class="token comment"># 这里面会出现一些float的数据，导致无法编码</span>
         doc_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> lbe<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>doc_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
     <span class="token keyword">else</span><span class="token punctuation">:</span>
         doc_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> lbe<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>doc_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
     feature_max_idx<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> doc_data<span class="token punctuation">[</span>feature<span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
     feature_encoder<span class="token punctuation">[</span>feature<span class="token punctuation">]</span> <span class="token operator">=</span> lbe

 data<span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> feature_encoder<span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transform<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 data<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span> <span class="token operator">=</span> feature_encoder<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transform<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


 <span class="token comment"># join 用户侧和新闻侧的特征</span>
 data <span class="token operator">=</span> data<span class="token punctuation">.</span>join<span class="token punctuation">(</span>user_data<span class="token punctuation">.</span>set_index<span class="token punctuation">(</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> on<span class="token operator">=</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">,</span> how<span class="token operator">=</span><span class="token string">&quot;inner&quot;</span><span class="token punctuation">)</span>
 data <span class="token operator">=</span> data<span class="token punctuation">.</span>join<span class="token punctuation">(</span>doc_data<span class="token punctuation">.</span>set_index<span class="token punctuation">(</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> on<span class="token operator">=</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">,</span> how<span class="token operator">=</span><span class="token string">&quot;inner&quot;</span><span class="token punctuation">)</span>

 sparse_features <span class="token operator">=</span> user_sparse_features <span class="token operator">+</span> doc_sparse_features
 dense_features <span class="token operator">=</span> doc_dense_features

 features <span class="token operator">=</span> sparse_features <span class="token operator">+</span> dense_features

 mms <span class="token operator">=</span> MinMaxScaler<span class="token punctuation">(</span>feature_range<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 data<span class="token punctuation">[</span>dense_features<span class="token punctuation">]</span> <span class="token operator">=</span> mms<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>data<span class="token punctuation">[</span>dense_features<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、配置特征以及模型训练 构建模型所需的输入特征，同时构建DSSM模型及训练。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>embedding_dim <span class="token operator">=</span> <span class="token number">8</span>
user_feature_columns <span class="token operator">=</span> <span class="token punctuation">[</span>SparseFeat<span class="token punctuation">(</span><span class="token string">&#39;user_id&#39;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;user_id&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&quot;gender&quot;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;gender&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&quot;age&quot;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;age&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&quot;device&quot;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;device&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&quot;os&quot;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;os&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&quot;province&quot;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;province&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&quot;city&quot;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;city&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token punctuation">]</span>

item_feature_columns <span class="token operator">=</span> <span class="token punctuation">[</span>SparseFeat<span class="token punctuation">(</span><span class="token string">&#39;article_id&#39;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;article_id&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     DenseFeat<span class="token punctuation">(</span><span class="token string">&#39;img_num&#39;</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                     DenseFeat<span class="token punctuation">(</span><span class="token string">&#39;title_len&#39;</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&#39;cate&#39;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;cate&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>
                     SparseFeat<span class="token punctuation">(</span><span class="token string">&#39;sub_cate&#39;</span><span class="token punctuation">,</span> feature_max_idx<span class="token punctuation">[</span><span class="token string">&#39;sub_cate&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">]</span>

model <span class="token operator">=</span> DSSM<span class="token punctuation">(</span>user_feature_columns<span class="token punctuation">,</span> item_feature_columns<span class="token punctuation">,</span>
          user_dnn_hidden_units<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> item_dnn_hidden_units<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># FM(user_feature_columns,item_feature_columns)</span>

model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">&quot;adagrad&quot;</span><span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token string">&quot;binary_crossentropy&quot;</span><span class="token punctuation">,</span> metrics<span class="token operator">=</span><span class="token punctuation">[</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>Recall<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>Precision<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token punctuation">)</span> <span class="token comment">#</span>

history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_model_input<span class="token punctuation">,</span> train_label<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> validation_split<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>3、生成embedding用于召回 利用训练过的模型获取所有item的embeddings，同时获取所有测试集的user embedding，保存之后用于之后的召回工作。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>all_item_model_input <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">:</span> item_profile<span class="token punctuation">[</span><span class="token string">&#39;article_id&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span>
                     <span class="token string">&quot;img_num&quot;</span><span class="token punctuation">:</span> item_profile<span class="token punctuation">[</span><span class="token string">&#39;img_num&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span>
                     <span class="token string">&quot;title_len&quot;</span><span class="token punctuation">:</span> item_profile<span class="token punctuation">[</span><span class="token string">&#39;title_len&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span>
                     <span class="token string">&quot;cate&quot;</span><span class="token punctuation">:</span> item_profile<span class="token punctuation">[</span><span class="token string">&#39;cate&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span>
                     <span class="token string">&quot;sub_cate&quot;</span><span class="token punctuation">:</span> item_profile<span class="token punctuation">[</span><span class="token string">&#39;sub_cate&#39;</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span><span class="token punctuation">}</span>

user_embedding_model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>model<span class="token punctuation">.</span>user_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>model<span class="token punctuation">.</span>user_embedding<span class="token punctuation">)</span>
item_embedding_model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token operator">=</span>model<span class="token punctuation">.</span>item_input<span class="token punctuation">,</span> outputs<span class="token operator">=</span>model<span class="token punctuation">.</span>item_embedding<span class="token punctuation">)</span>

user_embs <span class="token operator">=</span> user_embedding_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>test_user_model_input<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token number">12</span><span class="token punctuation">)</span>
item_embs <span class="token operator">=</span> item_embedding_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>all_item_model_input<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">2</span> <span class="token operator">**</span> <span class="token number">12</span><span class="token punctuation">)</span>

user_idx_2_rawid<span class="token punctuation">,</span> doc_idx_2_rawid <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>user_embs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 user_idx_2_rawid<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> test_user_model_input<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span>

 <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>item_embs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     doc_idx_2_rawid<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> all_item_model_input<span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span>

     <span class="token comment"># 保存一份</span>
     pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span><span class="token punctuation">(</span>user_embs<span class="token punctuation">,</span> user_idx_2_rawid<span class="token punctuation">,</span> feature_encoder<span class="token punctuation">[</span><span class="token string">&quot;user_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">open</span><span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&#39;user_embs.pkl&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;wb&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
     pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span><span class="token punctuation">(</span>item_embs<span class="token punctuation">,</span> doc_idx_2_rawid<span class="token punctuation">,</span> feature_encoder<span class="token punctuation">[</span><span class="token string">&quot;article_id&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">open</span><span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&#39;item_embs.pkl&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;wb&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="ann召回" tabindex="-1"><a class="header-anchor" href="#ann召回"><span>ANN召回</span></a></h3><p>1、为测试集用户召回 通过annoy tree为所有的item构建索引，并通过测试集中所有的user embedding为每个用户召回一定数量的item。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">get_DSSM_recall_res</span><span class="token punctuation">(</span>user_embs<span class="token punctuation">,</span> doc_embs<span class="token punctuation">,</span> user_idx_2_rawid<span class="token punctuation">,</span> doc_idx_2_rawid<span class="token punctuation">,</span> topk<span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token triple-quoted-string string">&quot;&quot;&quot;近邻检索，这里用annoy tree&quot;&quot;&quot;</span>
 <span class="token comment"># 把doc_embs构建成索引树</span>
 f <span class="token operator">=</span> user_embs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
 t <span class="token operator">=</span> AnnoyIndex<span class="token punctuation">(</span>f<span class="token punctuation">,</span> <span class="token string">&#39;angular&#39;</span><span class="token punctuation">)</span>
 <span class="token keyword">for</span> i<span class="token punctuation">,</span> v <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>doc_embs<span class="token punctuation">)</span><span class="token punctuation">:</span>
     t<span class="token punctuation">.</span>add_item<span class="token punctuation">(</span>i<span class="token punctuation">,</span> v<span class="token punctuation">)</span>
 t<span class="token punctuation">.</span>build<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>

 <span class="token comment"># 每个用户向量， 返回最近的TopK个item</span>
 user_recall_items_dict <span class="token operator">=</span> collections<span class="token punctuation">.</span>defaultdict<span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">)</span>
 <span class="token keyword">for</span> i<span class="token punctuation">,</span> u <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>user_embs<span class="token punctuation">)</span><span class="token punctuation">:</span>
     recall_doc_scores <span class="token operator">=</span> t<span class="token punctuation">.</span>get_nns_by_vector<span class="token punctuation">(</span>u<span class="token punctuation">,</span> topk<span class="token punctuation">,</span> include_distances<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
     <span class="token comment"># recall_doc_scores是(([doc_idx], [scores]))， 这里需要转成原始doc的id</span>
     raw_doc_scores <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>recall_doc_scores<span class="token punctuation">)</span>
     raw_doc_scores<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>doc_idx_2_rawid<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> raw_doc_scores<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
     <span class="token comment"># 转换成实际用户id</span>
     user_recall_items_dict<span class="token punctuation">[</span>user_idx_2_rawid<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span>raw_doc_scores<span class="token punctuation">)</span><span class="token punctuation">)</span>

 <span class="token comment"># 默认是分数从小到大排的序， 这里要从大到小</span>
 user_recall_items_dict <span class="token operator">=</span> <span class="token punctuation">{</span>k<span class="token punctuation">:</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>v<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> user_recall_items_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>

 pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>user_recall_items_dict<span class="token punctuation">,</span> <span class="token builtin">open</span><span class="token punctuation">(</span>file_path <span class="token operator">+</span> <span class="token string">&#39;DSSM_u2i_dict.pkl&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;wb&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

 <span class="token keyword">return</span> user_recall_items_dict
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、测试召回结果 为测试集用户的召回结果进行测试。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>user_recall_items_dict <span class="token operator">=</span> get_DSSM_recall_res<span class="token punctuation">(</span>user_embs<span class="token punctuation">,</span> item_embs<span class="token punctuation">,</span> user_idx_2_rawid<span class="token punctuation">,</span> doc_idx_2_rawid<span class="token punctuation">,</span> topk<span class="token operator">=</span>TOP_NUM<span class="token punctuation">)</span>

test_true_items <span class="token operator">=</span> <span class="token punctuation">{</span>line<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>line<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> test_set<span class="token punctuation">}</span>

s <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
precision <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> uid <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>user_recall_items_dict<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token comment"># try:</span>
 pred <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x<span class="token punctuation">,</span> _ <span class="token keyword">in</span> user_recall_items_dict<span class="token punctuation">[</span>uid<span class="token punctuation">]</span><span class="token punctuation">]</span>
 filter_item <span class="token operator">=</span> <span class="token boolean">None</span>
 recall_score <span class="token operator">=</span> recall_N<span class="token punctuation">(</span>test_true_items<span class="token punctuation">[</span>uid<span class="token punctuation">]</span><span class="token punctuation">,</span> pred<span class="token punctuation">,</span> N<span class="token operator">=</span>TOP_NUM<span class="token punctuation">)</span>
 s<span class="token punctuation">.</span>append<span class="token punctuation">(</span>recall_score<span class="token punctuation">)</span>
 precision_score <span class="token operator">=</span> precision_N<span class="token punctuation">(</span>test_true_items<span class="token punctuation">[</span>uid<span class="token punctuation">]</span><span class="token punctuation">,</span> pred<span class="token punctuation">,</span> N<span class="token operator">=</span>TOP_NUM<span class="token punctuation">)</span>
 precision<span class="token punctuation">.</span>append<span class="token punctuation">(</span>precision_score<span class="token punctuation">)</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;recall&quot;</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;precision&quot;</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>precision<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2>`,32),A={href:"https://zhuanlan.zhihu.com/p/165064102",target:"_blank",rel:"noopener noreferrer"},j={href:"https://mp.weixin.qq.com/s/aorZ43WozKrD2AudR6AnOg",target:"_blank",rel:"noopener noreferrer"},P={href:"https://zhuanlan.zhihu.com/p/358450850",target:"_blank",rel:"noopener noreferrer"},T={href:"https://dl.acm.org/doi/10.1145/3298689.3346996",target:"_blank",rel:"noopener noreferrer"},B={href:"https://zhuanlan.zhihu.com/p/358779957",target:"_blank",rel:"noopener noreferrer"},C={href:"https://zhuanlan.zhihu.com/p/430503952",target:"_blank",rel:"noopener noreferrer"},$={href:"https://zhuanlan.zhihu.com/p/441597009",target:"_blank",rel:"noopener noreferrer"},H={href:"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf",target:"_blank",rel:"noopener noreferrer"};function R(W,Z){const a=l("ExternalLinkIcon");return o(),c("div",null,[r,m,k,d,g,h,v,_,b,s("p",null,[n("此外张俊林大佬还将SENet应用于双塔模型中"),s("a",y,[n("(SENet双塔模型：在推荐领域召回粗排的应用及其它)"),t(a)]),n("，模型结构如下所示：")]),q,n(" 如上图所示，在user侧和item侧分别通过多个通道(DNN结构)为每个任务得到一个user embedding和item embedding，然后针对不同的目标分别计算user 和 item 的相似度，并计算各个目标的损失，最后的优化目标可以是多个任务损失之和，或者使用多任务学习中的动态损失权重。 "),f,x,w,z,s("p",null,[n("在"),s("a",M,[n("Google的双塔召回模型"),t(a)]),n("中，重点介绍了两个trick，将user和item侧输出的embedding进行归一化以及对于內积值除以温度系数，实验证明这两种方式可以取得十分好的效果。那为什么这两种方法会使得模型的效果更好呢？")]),N,S,s("p",null,[n("例如Airbnb根据业务逻辑来采样一些hard negative （增加与正样本同城的房间作为负样本，增强了正负样本在地域上的相似性；增加与正样本同城的房间作为负样本，增强了正负样本在地域上的相似性，），详细内容可以查看"),s("a",D,[n("原文"),t(a)])]),s("p",null,[n('例如百度和facebook依靠模型自己来挖掘Hard Negative，都是用上一版本的召回模型筛选出"没那么相似"的<user,item>对，作为额外负样本，用于训练下一版本召回模型。 详细可以查看'),s("a",E,[n("Mobius"),t(a)]),n(" 和 "),s("a",F,[n("EBR"),t(a)])]),L,s("ul",null,[s("li",null,[s("p",null,[s("a",A,[n("负样本为王：评Facebook的向量化召回算法"),t(a)])])]),s("li",null,[s("p",null,[s("a",j,[n("多目标DSSM召回实战"),t(a)])])]),s("li",null,[s("p",null,[s("a",P,[n("召回模型中的负样本构造"),t(a)])])]),s("li",null,[s("p",null,[s("a",T,[n("Youtube双塔模型"),t(a)])])]),s("li",null,[s("p",null,[s("a",B,[n("张俊林：SENet双塔模型：在推荐领域召回粗排的应用及其它"),t(a)])])]),s("li",null,[s("p",null,[s("a",C,[n("双塔召回模型的前世今生（上篇）"),t(a)])])]),s("li",null,[s("p",null,[s("a",$,[n("双塔召回模型的前世今生（下篇）"),t(a)])])]),s("li",null,[s("p",null,[s("a",H,[n("Learning Deep Structured Semantic Models for Web Search using Clickthrough Data"),t(a)])])])])])}const V=e(u,[["render",R],["__file","ch2.2.4_dssm.html.vue"]]),O=JSON.parse('{"path":"/rcmd/ch02/ch2.2/ch2.2.4_dssm.html","title":"双塔召回","lang":"zh-CN","frontmatter":{"date":"2024-06-13T00:00:00.000Z","title":"双塔召回","author":"Genhiy","order":4,"category":["推荐系统"],"tag":["无标签"],"feed":false,"seo":false,"head":[]},"headers":[{"level":2,"title":"双塔召回模型","slug":"双塔召回模型","link":"#双塔召回模型","children":[{"level":3,"title":"经典双塔模型","slug":"经典双塔模型","link":"#经典双塔模型","children":[]},{"level":3,"title":"SENet双塔模型","slug":"senet双塔模型","link":"#senet双塔模型","children":[]},{"level":3,"title":"多目标的双塔模型","slug":"多目标的双塔模型","link":"#多目标的双塔模型","children":[]}]},{"level":2,"title":"双塔模型的细节","slug":"双塔模型的细节","link":"#双塔模型的细节","children":[{"level":3,"title":"归一化与温度系数","slug":"归一化与温度系数","link":"#归一化与温度系数","children":[]},{"level":3,"title":"模型的应用","slug":"模型的应用","link":"#模型的应用","children":[]},{"level":3,"title":"负样本采样","slug":"负样本采样","link":"#负样本采样","children":[]}]},{"level":2,"title":"代码实现","slug":"代码实现","link":"#代码实现","children":[{"level":3,"title":"模型训练数据","slug":"模型训练数据","link":"#模型训练数据","children":[]},{"level":3,"title":"DSSM 模型","slug":"dssm-模型","link":"#dssm-模型","children":[]},{"level":3,"title":"模型训练","slug":"模型训练","link":"#模型训练","children":[]},{"level":3,"title":"ANN召回","slug":"ann召回","link":"#ann召回","children":[]}]},{"level":2,"title":"参考资料","slug":"参考资料","link":"#参考资料","children":[]}],"git":{},"readingTime":{"minutes":24.42,"words":7326},"filePathRelative":"rcmd/ch02/ch2.2/ch2.2.4_dssm.md","localizedDate":"2024年6月13日"}');export{V as comp,O as data};
