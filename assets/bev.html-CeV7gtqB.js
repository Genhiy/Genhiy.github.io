import{_ as p}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as o,o as c,c as l,a as n,d as s,b as t,e}from"./app-SD3SAAIy.js";const i="/assets/images/bev/Snipaste_2024-05-17_13-36-03.png",u="/assets/images/bev/Snipaste_2024-05-17_12-12-10.png",r={},k=e(`<h2 id="bevformer" tabindex="-1"><a class="header-anchor" href="#bevformer"><span>BEVFormer</span></a></h2><h3 id="pipeline流程" tabindex="-1"><a class="header-anchor" href="#pipeline流程"><span>PipeLine流程</span></a></h3><p>其实和大多数的算法流程一样，BEVFormer的pipeline流程如下：</p><ul><li>使用Backbone和Neck（ResNet-101-DCN + FPN）提取环视图像的多尺度特征。</li><li>Encoder模块（包括Temporal Self-Attention模块和Spatial Cross-Attention模块），通过论文提出的方法将环视图像特征转换为BEV特征。</li><li>类似于Deformable DETR的Decoder模块，完成3D目标检测的分类和定位任务。</li><li>正负样本的定义采用了Transformer中常用的匈牙利匹配算法，使用Focal Loss + L1 Loss作为总损失，并最小化该损失。</li><li>损失的计算使用Focal Loss分类损失和L1 Loss回归损失，并进行反向传播和更新网络模型参数。</li></ul><h3 id="输入数据格式" tabindex="-1"><a class="header-anchor" href="#输入数据格式"><span>输入数据格式</span></a></h3><p>对于BEVFormer网络模型，输入数据是一个6维张量：（bs，queue，cam，C，H，W）。其中：</p><ul><li>bs表示batch size大小；</li><li>queue表示连续帧的数量。由于BEVFormer采用了时序信息的思想（我认为加入时序信息后，可以一定程度上缓解遮挡问题），因此输入到网络模型中的数据要包括之前几帧的数据，而不仅仅是当前帧的数据；</li><li>cam表示每帧中包含的图像数量。在nuScenes数据集中，一辆车通常带有六个环视相机传感器，可以实现360度全场景的覆盖，因此一帧会包含六张环视图片；</li><li>C，H，W分别表示图片的通道数、高度和宽度。</li></ul><h3 id="网络特征提取" tabindex="-1"><a class="header-anchor" href="#网络特征提取"><span>网络特征提取</span></a></h3><p>网络特征提取的目的是将每一帧对应的六张环视图像的特征提取出来，便于后续转换到 BEV 特征空间，生成 BEV 特征，在特征提取过程中，tensor流的变换情况如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># 输入图片信息 tensor: (bs, queue, cam, c, h, w)</span>
<span class="token comment"># 通过 for loop 方式一次获取单帧对应的六张环视图像</span>
<span class="token comment"># 送入到 Backbone + Neck 网络提取多尺度的图像特征</span>

<span class="token keyword">for</span> idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 利用除当前帧之外的所有帧迭代计算 \`prev_bev\` 特征</span>
    single_frame <span class="token operator">=</span> tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> idx<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>   <span class="token comment"># (bs, cam, c, h, w)</span>

    <span class="token comment"># 将 bs * cam 看作是 batch size，将原张量 reshape 成 4 维的张量</span>
    <span class="token comment"># 待 Backbone + Neck 网络提取多尺度特征后，再把 bs * cam 的维度再拆成 bs，cam</span>

    single_frame <span class="token operator">=</span> single_frame<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bs <span class="token operator">*</span> cam<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
    feats <span class="token operator">=</span> Backbone<span class="token punctuation">(</span>FPN<span class="token punctuation">(</span>single_frame<span class="token punctuation">)</span><span class="token punctuation">)</span> 

    <span class="token triple-quoted-string string">&quot;&quot;&quot; feats 是一个多尺度的特征列表 &quot;&quot;&quot;</span>
    <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>bs<span class="token punctuation">,</span> cam<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> h <span class="token operator">/</span> <span class="token number">8</span><span class="token punctuation">,</span> w <span class="token operator">/</span> <span class="token number">8</span><span class="token punctuation">)</span>
    <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>bs<span class="token punctuation">,</span> cam<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> h <span class="token operator">/</span> <span class="token number">16</span><span class="token punctuation">,</span> w <span class="token operator">/</span> <span class="token number">16</span><span class="token punctuation">)</span>
    <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>bs<span class="token punctuation">,</span> cam<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> h <span class="token operator">/</span> <span class="token number">32</span><span class="token punctuation">,</span> w <span class="token operator">/</span> <span class="token number">32</span><span class="token punctuation">)</span>
    <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>bs<span class="token punctuation">,</span> cam<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> h <span class="token operator">/</span> <span class="token number">64</span><span class="token punctuation">,</span> w <span class="token operator">/</span> <span class="token number">64</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="bev-特征产生" tabindex="-1"><a class="header-anchor" href="#bev-特征产生"><span>BEV 特征产生</span></a></h3><p>生成 BEV 特征的过程中，最核心的部分是论文中提出的 Encoder 模块，其中包括 Spatial Cross-Attention 和 Temporal Self-Attention。在这两个模块中，都使用了一个非常关键的组件：多尺度可变形注意力模块。这个模块将 Transformer 的全局注意力变为局部注意力，以减少训练时间并提高 Transformer 的收敛速度。（该思想最早出现在 Deformable DETR 中）</p><figure><img src="`+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="temporal-self-attention" tabindex="-1"><a class="header-anchor" href="#temporal-self-attention"><span>Temporal Self-Attention</span></a></h4><p>作用是将时序信息（如插图中的历史 BEV）与当前时刻的 BEV Query 进行融合，以提高 BEV Query 的建模能力。</p><figure><img src="'+u+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>对于 Temporal Self-Attention 模块而言，需要 bev_query、bev_pos、prev_bev、ref_point、value等参数。</p><ul><li>参数 bev_query：一个完全 learnable parameter，通过 nn.Embedding() 函数得到，形状 shape = (200 * 200，256)；200，200 分别代表 BEV 特征平面的长和宽；</li><li>参数 bev_pose：感觉也是一个完全 learnable parameter，与 2D 检测中常见的正余弦编码方式不同，感觉依旧是把不同的 grid 位置映射到一个高维的向量空间，shape = （bs，256，200，200）。</li><li>参数 ref_point：这个参数根据当前 Temporal Self-Attention 模块是否有 prev_bev 特征输入而言，会对应不同的情况，之所以会出现不同，是考虑到了前后时刻 BEV 特征存在特征不对齐的问题，BEV 特征不对齐主要体现在车自身不断运动以及车周围物体也在一定范围内运动。对于 Temporal Self-Attention 模块没有输入 prev_bev（第一帧没有前一时刻的 BEV 特征）的情况，其 ref_point = ref_2d；对于存在输入 prev_bev 的情况，其 ref_point = ref_2d + shift；</li><li>参数 value：对应着bev_query去查询的特征；对于 Temporal Self-Attention 模块输入包含 prev_bev时，<code>value = [prev_bev，bev_query]</code>，对应的参考点 <code>ref_point = [ref_2d + shift，ref_2d]</code>；如果输入不包含 prev_bev时，<code>value = [bev_query，bev_query]</code>，对应的参考点<code>ref_point = [ref_2d，ref_2d]</code>。</li></ul><p>输出bev query：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token triple-quoted-string string">&quot;&quot;&quot; 各个参数的 shape 情况 
1. value: (2，40000，8，32） 
    # 2: 代表前一时刻的 BEV 特征和后一时刻的 BEV 特征，两个特征在计算的过程中是互不干扰的，
    # 40000: 代表 bev_query 200 * 200 空间大小的每个位置
    # 8: 代表8个头，# 32: 每个头表示为 32 维的特征
2. spatial_shapes: (200, 200) # 方便将归一化的 sampling_locations 反归一化
3. level_start_index: 0 # BEV 特征只有一层
4. sampling_locations: (2, 40000, 8, 1, 4, 2)
5. attention_weights: (2, 40000, 8, 1, 4)
6. output: (2, 40000, 8, 32)
&quot;&quot;&quot;</span>
output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">,</span> level_start_index<span class="token punctuation">,</span> sampling_locations<span class="token punctuation">,</span>attention_weights<span class="token punctuation">,</span> self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span>

<span class="token string">&quot;&quot;</span>&quot; 最后将前一时刻的 bev_query 与当前时刻的 bev_query 做平均
output <span class="token operator">=</span> output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> <span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>bs<span class="token punctuation">]</span> <span class="token operator">+</span> output<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> bs<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">/</span>self<span class="token punctuation">.</span>num_bev_queue
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="spatial-cross-attention" tabindex="-1"><a class="header-anchor" href="#spatial-cross-attention"><span>Spatial Cross-Attention</span></a></h4><p>作用是利用 Temporal Self-Attention 模块输出的 bev_query，对主干网络和 Neck 网络提取到的多尺度环视图像特征进行查询，生成 BEV 空间下的 BEV Embedding 特征。</p><p>对于 Spatial Cross-Attention 模块而言，与 Temporal Self-Attention 模块需要的参数很类似，但是并不需要 bev_pos 参数，只需要 bev_query、ref_point、value（就是 concat 到一起的多尺度特征）；</p><ul><li>参数bev_query：bev_query参数来自于 Temporal Self-Attention 模块的输出；</li><li>参数queries_rebatch：之前也有提到，并不是 BEV 坐标系下的每个三维坐标都会映射到环视相机的所有图像上，而只会映射到其中的某几张图片上，所以使用所有来自 Temporal Self-Attention 模块的所有bev_query会消耗很大的计算量，所以这里是对bev_query进行了重新的整合。</li><li>参数value：对于 Transformer 而言，由于其本身是处理文本序列的模型，而文本序列都是一组组一维的数据，所以需要将前面提取的多尺度特征做 flatten() 处理，并将所有层的特征汇聚到一起，方便之后做查询；</li></ul><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token triple-quoted-string string">&quot;&quot;&quot; 首先将多尺度的特征每一层都进行 flatten() &quot;&quot;&quot;</span>
<span class="token keyword">for</span> lvl<span class="token punctuation">,</span> feat <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>mlvl_feats<span class="token punctuation">)</span><span class="token punctuation">:</span>
    bs<span class="token punctuation">,</span> num_cam<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> feat<span class="token punctuation">.</span>shape
    spatial_shape <span class="token operator">=</span> <span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
    feat <span class="token operator">=</span> feat<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_cams_embeds<span class="token punctuation">:</span>
        feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>cams_embeds<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        feat <span class="token operator">=</span> feat <span class="token operator">+</span> self<span class="token punctuation">.</span>level_embeds<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> lvl<span class="token punctuation">:</span>lvl <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>feat<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
        spatial_shapes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spatial_shape<span class="token punctuation">)</span>
        feat_flatten<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feat<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&quot;&quot;&quot; 对每个 camera 的所有层级特征进行汇聚 &quot;&quot;&quot;</span>
feat_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>feat_flatten<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># (cam, bs, sum(h*w), 256)</span>
spatial_shapes <span class="token operator">=</span> torch<span class="token punctuation">.</span>as_tensor<span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>bev_pos<span class="token punctuation">.</span>device<span class="token punctuation">)</span>

<span class="token comment"># 计算每层特征的起始索引位置</span>
level_start_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>spatial_shapes<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">.</span>prod<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 维度变换</span>
feat_flatten <span class="token operator">=</span> feat_flatten<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># (num_cam, sum(H*W), bs, embed_dims)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>输出bev_embedding：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token triple-quoted-string string">&quot;&quot;&quot;
1. value: shape = (cam = 6, sum(h_i * w_i) = 30825, head = 8, dim = 32)
2. spatial_shapes = ([[116, 200], [58, 100], [29,  50], [15,  25]])
3. level_start_index= [0, 23200, 29000, 30450]
4. sampling_locations = (cam, max_len, 8, 4, 8, 2)
5. attention_weights = (cam, max_len, 8, 4, 8)

6. output = (cam, max_len, 8, 32)
&quot;&quot;&quot;</span>
output <span class="token operator">=</span> MultiScaleDeformableAttnFunction<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>value<span class="token punctuation">,</span> spatial_shapes<span class="token punctuation">,</span> level_start_index<span class="token punctuation">,</span> sampling_locations<span class="token punctuation">,</span>
                attention_weights<span class="token punctuation">,</span> self<span class="token punctuation">.</span>im2col_step<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&quot;&quot;&quot;最后再将六个环视相机查询到的特征整合到一起，再求一个平均值 &quot;&quot;&quot;</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> index_query_per_img <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>indexes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># slots: (bs, 40000, 256)</span>
        slots<span class="token punctuation">[</span>j<span class="token punctuation">,</span> index_query_per_img<span class="token punctuation">]</span> <span class="token operator">+=</span> queries<span class="token punctuation">[</span>j <span class="token operator">*</span> self<span class="token punctuation">.</span>num_cams <span class="token operator">+</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>index_query_per_img<span class="token punctuation">)</span><span class="token punctuation">]</span>

count <span class="token operator">=</span> bev_mask<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span>
count <span class="token operator">=</span> count<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
count <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>count<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
slots <span class="token operator">=</span> slots <span class="token operator">/</span> count<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>  <span class="token comment"># maybe normalize.</span>
slots <span class="token operator">=</span> self<span class="token punctuation">.</span>output_proj<span class="token punctuation">(</span>slots<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>将 Temporal Self-Attetion 模块和 Spatial Cross-Attention 模块堆叠在一起，并重复六次，最终得到的 BEV Embedding 特征作为下游 3D 目标检测和道路分割任务的 BEV 空间特征。</p>`,28),d={href:"https://zhuanlan.zhihu.com/p/543335939",target:"_blank",rel:"noopener noreferrer"},m=e(`<h3 id="decoder模块" tabindex="-1"><a class="header-anchor" href="#decoder模块"><span>Decoder模块</span></a></h3><p>以上过程中，利用了当前帧之前所有帧的特征迭代修正，以获得prev_bev的特征，因此在使用 Decoder 模块进行解码之前，需要对当前时刻的 6 张环视图片同样使用 Backbone + Neck 提取多尺度特征，并使用上述 Temporal Self-Attention 模块和 Spatial Cross-Attention 模块的逻辑来生成当前时刻的bev_embedding特征。然后，将这部分特征输入到 Decoder 中进行 3D 目标检测。</p><p>分类分支的网络结构：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>Sequential<span class="token punctuation">(</span>
  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> LayerNorm<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">,</span> elementwise_affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>回归分支的网络结构：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>Sequential<span class="token punctuation">(</span>
  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="算法创新" tabindex="-1"><a class="header-anchor" href="#算法创新"><span>算法创新</span></a></h3><h4 id="使用transformer和时间结构来聚合时空信息" tabindex="-1"><a class="header-anchor" href="#使用transformer和时间结构来聚合时空信息"><span>使用Transformer和时间结构来聚合时空信息</span></a></h4><p>BEVFormer使用Transformer和时间结构来聚合来自多视角摄像机和历史BEV特征的时空信息。</p><p>具体来说，BEVFormer使用预定义的网格状BEV查询与空间/时间特征进行交互，以查找并聚合时空信息。这种方法可以有效地捕获3D场景中物体的时空关系，并生成更强大的表示。</p><h4 id="使用查询来查找空间-时间空间并相应地聚合时空信息" tabindex="-1"><a class="header-anchor" href="#使用查询来查找空间-时间空间并相应地聚合时空信息"><span>使用查询来查找空间/时间空间并相应地聚合时空信息</span></a></h4><p>除了使用Transformer和时间结构来聚合时空信息外，BEVFormer还使用查询来查找空间/时间空间并相应地聚合时空信息。</p><p>具体而言，BEVFormer使用两种类型的注意力机制：一种是用于跨摄像机视图之间的注意力机制（即“Spatial Cross-Attention”），另一种是用于历史BEV特征之间的注意力机制（即“Temporal Self-Attention”）。</p><p>这些注意力机制可以帮助BEVFormer有效地捕获3D场景中物体之间的关系，并生成更好的表征。</p><h4 id="适用于多个3d感知任务" tabindex="-1"><a class="header-anchor" href="#适用于多个3d感知任务"><span>适用于多个3D感知任务</span></a></h4><p>从BEVFormer生成的BEV特征可以同时支持多个3D感知任务，例如3D物体检测和地图分割。</p><p>这意味着，使用BEVFormer可以减少需要为不同任务训练不同模型的工作量，并提高系统整体性能。</p>`,17),b={href:"https://blog.csdn.net/KANG157/article/details/130673832",target:"_blank",rel:"noopener noreferrer"},v=n("h2",{id:"simplebev",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#simplebev"},[n("span",null,"SimpleBEV")])],-1),h=n("p",null,"核心点：BEV的基础框架",-1),_={href:"https://github.com/aharley/simple_bev/tree/main",target:"_blank",rel:"noopener noreferrer"},f=n("h2",{id:"fiety",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#fiety"},[n("span",null,"Fiety")])],-1),g=n("p",null,"核心点：训练和推断未来鸟瞰图",-1),y={href:"https://github.com/wayveai/fiery/tree/master",target:"_blank",rel:"noopener noreferrer"},q={href:"https://blog.csdn.net/weixin_44580210/article/details/127605230",target:"_blank",rel:"noopener noreferrer"};function E(B,V){const a=o("ExternalLinkIcon");return c(),l("div",null,[k,n("blockquote",null,[n("p",null,[s("具体代码详解："),n("a",d,[s("万字长文理解纯视觉感知算法 —— BEVFormer"),t(a)])])]),m,n("blockquote",null,[n("p",null,[s("参考资料："),n("a",b,[s("BEV专栏（一）从BEVFormer深入探究BEV流程（上篇）"),t(a)])])]),v,h,n("blockquote",null,[n("p",null,[s("源代码："),n("a",_,[s("https://github.com/aharley/simple_bev/tree/main"),t(a)])])]),f,g,n("blockquote",null,[n("p",null,[s("源代码："),n("a",y,[s("https://github.com/wayveai/fiery/tree/master"),t(a)])])]),n("blockquote",null,[n("p",null,[s("其余可参考内容记录： "),n("a",q,[s("计算机视觉算法——BEV Perception算法总结（3D LaneNet / LSS / PON / BEVFormer / GKT / Translating Image to Maps）"),t(a)])])])])}const S=p(r,[["render",E],["__file","bev.html.vue"]]),N=JSON.parse('{"path":"/posts/bev.html","title":"BEV","lang":"zh-CN","frontmatter":{"date":"2024-05-17T00:00:00.000Z","title":"BEV","author":"Genhiy","order":1,"category":["自动驾驶"],"tag":["BEV"],"star":10,"description":"BEVFormer PipeLine流程 其实和大多数的算法流程一样，BEVFormer的pipeline流程如下： 使用Backbone和Neck（ResNet-101-DCN + FPN）提取环视图像的多尺度特征。 Encoder模块（包括Temporal Self-Attention模块和Spatial Cross-Attention模块），通过...","head":[["meta",{"property":"og:url","content":"https://github.com/Genhiy/Genhiy.github.io/posts/bev.html"}],["meta",{"property":"og:site_name","content":"Genhiy"}],["meta",{"property":"og:title","content":"BEV"}],["meta",{"property":"og:description","content":"BEVFormer PipeLine流程 其实和大多数的算法流程一样，BEVFormer的pipeline流程如下： 使用Backbone和Neck（ResNet-101-DCN + FPN）提取环视图像的多尺度特征。 Encoder模块（包括Temporal Self-Attention模块和Spatial Cross-Attention模块），通过..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"Genhiy"}],["meta",{"property":"article:tag","content":"BEV"}],["meta",{"property":"article:published_time","content":"2024-05-17T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"BEV\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-05-17T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Genhiy\\"}]}"]]},"headers":[{"level":2,"title":"BEVFormer","slug":"bevformer","link":"#bevformer","children":[{"level":3,"title":"PipeLine流程","slug":"pipeline流程","link":"#pipeline流程","children":[]},{"level":3,"title":"输入数据格式","slug":"输入数据格式","link":"#输入数据格式","children":[]},{"level":3,"title":"网络特征提取","slug":"网络特征提取","link":"#网络特征提取","children":[]},{"level":3,"title":"BEV 特征产生","slug":"bev-特征产生","link":"#bev-特征产生","children":[]},{"level":3,"title":"Decoder模块","slug":"decoder模块","link":"#decoder模块","children":[]},{"level":3,"title":"算法创新","slug":"算法创新","link":"#算法创新","children":[]}]},{"level":2,"title":"SimpleBEV","slug":"simplebev","link":"#simplebev","children":[]},{"level":2,"title":"Fiety","slug":"fiety","link":"#fiety","children":[]}],"git":{},"readingTime":{"minutes":8.86,"words":2657},"filePathRelative":"posts/bev.md","localizedDate":"2024年5月17日","excerpt":"<h2>BEVFormer</h2>\\n<h3>PipeLine流程</h3>\\n<p>其实和大多数的算法流程一样，BEVFormer的pipeline流程如下：</p>\\n<ul>\\n<li>使用Backbone和Neck（ResNet-101-DCN + FPN）提取环视图像的多尺度特征。</li>\\n<li>Encoder模块（包括Temporal Self-Attention模块和Spatial Cross-Attention模块），通过论文提出的方法将环视图像特征转换为BEV特征。</li>\\n<li>类似于Deformable DETR的Decoder模块，完成3D目标检测的分类和定位任务。</li>\\n<li>正负样本的定义采用了Transformer中常用的匈牙利匹配算法，使用Focal Loss + L1 Loss作为总损失，并最小化该损失。</li>\\n<li>损失的计算使用Focal Loss分类损失和L1 Loss回归损失，并进行反向传播和更新网络模型参数。</li>\\n</ul>","autoDesc":true}');export{S as comp,N as data};
